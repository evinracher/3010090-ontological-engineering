{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evinracher/3010090-ontological-engineering/blob/main/week3/part1/3_02_Ejemplo_Memoria_Largo_Plazo_LangChain_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECSQOecs-Tcr"
      },
      "source": [
        "# ðŸ§  Memoria a Largo Plazo en Agentes (LangChain + LangGraph)\n",
        "\n",
        "Colab-ready. Cubre: amnesia, corto vs largo, 4 pilares (Storage/Extraction/Indexing/Retrieval), tÃ©cnicas de almacenamiento (relacional/vector/NoSQL/hÃ­brido), implementaciÃ³n en LangChain, checkpointing en LangGraph, estrategia hÃ­brida y mejores prÃ¡cticas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziHYvIa3-Tcv"
      },
      "source": [
        "## 0) Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkjIfPHQ-Tcv"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "!pip -q install \"langchain>=1,<2\" langchain-core langchain-community\n",
        "!pip -q install langchain-groq langsmith chromadb langgraph pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fyRT2rs-Tcx"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "if not os.getenv(\"GROQ_API_KEY\"):\n",
        "    try:\n",
        "        os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"GROQ_API_KEY:\", \"âœ…\" if os.getenv(\"GROQ_API_KEY\") else \"âš ï¸\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGSMK5Wk-Tcx"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.2)\n",
        "print(\"âœ… LLM listo:\", llm.model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXz-RoPg-Tcy"
      },
      "source": [
        "## 1) El problema de la amnesia (sin memoria persistente)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcLlxC5V-Tcy"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "def one_shot(user_text: str):\n",
        "    return llm.invoke([SystemMessage(content=\"Eres un asistente.\"), HumanMessage(content=user_text)]).content\n",
        "\n",
        "print(\"Lunes:\", one_shot(\"Me llamo Carlos y trabajo en marketing digital.\"))\n",
        "print(\"Viernes:\", one_shot(\"Â¿QuÃ© cursos me recomiendas para mi profesiÃ³n?\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHF48qB3-Tcz"
      },
      "source": [
        "## 2) Corto vs Largo plazo\n",
        "\n",
        "- **Corto**: historial de la sesiÃ³n actual.\n",
        "- **Largo**: hechos persistentes + recuerdos recuperables (dÃ­as/semanas).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXJnhucX-Tcz"
      },
      "source": [
        "## 3) Los 4 pilares: Storage, Extraction, Indexing, Retrieval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C3TYZhP-Tc0"
      },
      "source": [
        "## - Storage (hechos estructurados) con SQLite\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H3r6Hsd-Tc0"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import sqlite3, time, json\n",
        "\n",
        "conn = sqlite3.connect(\"memoria_struct.db\")\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS user_facts (\n",
        "  user_id TEXT,\n",
        "  key TEXT,\n",
        "  value TEXT,\n",
        "  ts INTEGER,\n",
        "  PRIMARY KEY (user_id, key)\n",
        ")\n",
        "\"\"\")\n",
        "conn.commit()\n",
        "\n",
        "def upsert_fact(user_id: str, key: str, value: str):\n",
        "    cur.execute(\n",
        "        \"INSERT OR REPLACE INTO user_facts(user_id,key,value,ts) VALUES (?,?,?,?)\",\n",
        "        (user_id, key, value, int(time.time()))\n",
        "    )\n",
        "    conn.commit()\n",
        "\n",
        "def get_facts(user_id: str):\n",
        "    rows = cur.execute(\"SELECT key, value, ts FROM user_facts WHERE user_id=?\", (user_id,)).fetchall()\n",
        "    return {k: v for (k, v, _) in rows}\n",
        "\n",
        "upsert_fact(\"carlos\", \"nombre\", \"Carlos\")\n",
        "upsert_fact(\"carlos\", \"profesion\", \"marketing digital\")\n",
        "print(get_facts(\"carlos\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBdclPtQ-Tc0"
      },
      "source": [
        "## - Storage + Indexing (memoria semÃ¡ntica) con Vector Store (Chroma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install sentence-transformers"
      ],
      "metadata": {
        "id": "D4eU17yhAMMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "emb = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "vectordb = Chroma(\n",
        "    collection_name=\"memoria_conversaciones\",\n",
        "    embedding_function=emb,\n",
        "    persist_directory=None  # in-memory\n",
        ")\n",
        "\n",
        "vectordb.add_documents([\n",
        "    Document(page_content=\"Carlos quiere mejorar SEO.\", metadata={\"user_id\":\"carlos\",\"ts\":1}),\n",
        "    Document(page_content=\"Carlos prefiere cursos con proyectos prÃ¡cticos.\", metadata={\"user_id\":\"carlos\",\"ts\":2}),\n",
        "    Document(page_content=\"Carlos estÃ¡ interesado en analÃ­tica web.\", metadata={\"user_id\":\"carlos\",\"ts\":3}),\n",
        "])\n",
        "\n",
        "print(\"âœ… Docs indexados:\", vectordb._collection.count())\n"
      ],
      "metadata": {
        "id": "r4KHIRFe_qe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siPDOm2g-Tc1"
      },
      "source": [
        "## - Retrieval: traer recuerdos relevantes (filtrando por usuario)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aa-13OX-Tc2"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "q = \"Â¿QuÃ© le interesa aprender a Carlos?\"\n",
        "hits = vectordb.similarity_search(q, k=2, filter={\"user_id\":\"carlos\"})\n",
        "for d in hits:\n",
        "    print(\"-\", d.page_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcwfe8gQ-Tc2"
      },
      "source": [
        "## - Extraction: extraer hechos valiosos con salida estructurada\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALhXymcn-Tc2"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "class ExtractedFacts(BaseModel):\n",
        "    nombre: Optional[str] = Field(default=None)\n",
        "    profesion: Optional[str] = Field(default=None)\n",
        "    preferencia_aprendizaje: Optional[str] = Field(default=None)\n",
        "\n",
        "extract_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Extrae SOLO hechos estables del usuario para memoria a largo plazo. \"\n",
        "     \"No incluyas saludos ni informaciÃ³n temporal. Devuelve SOLO JSON.\"),\n",
        "    (\"user\", \"ConversaciÃ³n:\\n{conversation}\")\n",
        "])\n",
        "\n",
        "extract_chain = extract_prompt | llm.with_structured_output(ExtractedFacts)\n",
        "\n",
        "conversation = \"\"\"\n",
        "Usuario: Hola, soy Carlos. Trabajo en marketing digital.\n",
        "Usuario: Me gustan cursos con proyectos prÃ¡cticos y casos reales.\n",
        "Usuario: Esta semana estoy estresado por una entrega.\n",
        "\"\"\"\n",
        "\n",
        "facts = extract_chain.invoke({\"conversation\": conversation})\n",
        "print(facts)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqOVCt3m-Tc2"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "for k, v in facts.model_dump().items():\n",
        "    if v:\n",
        "        upsert_fact(\"carlos\", k, v)\n",
        "\n",
        "print(\"Hechos persistidos:\", get_facts(\"carlos\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I76whCX-Tc2"
      },
      "source": [
        "## - LangChain: responder usando hechos + recuerdos recuperados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxV2iuqD-Tc2"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import json\n",
        "\n",
        "assistant_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Eres un asistente que recomienda cursos.\\n\"\n",
        "     \"Hechos (estructurados): {facts}\\n\"\n",
        "     \"Recuerdos (semÃ¡nticos):\\n{memories}\\n\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "def answer_with_ltm(user_id: str, question: str):\n",
        "    f = get_facts(user_id)\n",
        "    sem = vectordb.similarity_search(question, k=2, filter={\"user_id\": user_id})\n",
        "    memories = \"\\n\".join([f\"- {d.page_content}\" for d in sem]) if sem else \"(sin recuerdos)\"\n",
        "    chain = assistant_prompt | llm | StrOutputParser()\n",
        "    return chain.invoke({\"facts\": json.dumps(f, ensure_ascii=False), \"memories\": memories, \"question\": question})\n",
        "\n",
        "print(answer_with_ltm(\"carlos\", \"Â¿QuÃ© cursos me recomiendas para mi profesiÃ³n?\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px1yLBN4-Tc2"
      },
      "source": [
        "## 4) LangGraph: checkpointing (memoria de sesiÃ³n persistente)\n",
        "\n",
        "Checkpointing persiste el **estado** del grafo (corto plazo entre ejecuciones).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9mGyVnm-Tc3"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from typing import TypedDict, List\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "\n",
        "class ChatState(TypedDict):\n",
        "    messages: List[BaseMessage]\n",
        "\n",
        "def chat_node(state: ChatState):\n",
        "    resp = llm.invoke(state[\"messages\"])\n",
        "    return {\"messages\": state[\"messages\"] + [AIMessage(content=resp.content)]}\n",
        "\n",
        "g = StateGraph(ChatState)\n",
        "g.add_node(\"chat\", chat_node)\n",
        "g.set_entry_point(\"chat\")\n",
        "g.add_edge(\"chat\", END)\n",
        "\n",
        "app = g.compile(checkpointer=MemorySaver())\n",
        "thread_id = \"thread-carlos-1\"\n",
        "\n",
        "out1 = app.invoke({\"messages\":[HumanMessage(content=\"Me llamo Carlos y trabajo en marketing digital.\")]},\n",
        "                  config={\"configurable\":{\"thread_id\": thread_id}})\n",
        "print(out1[\"messages\"][-1].content)\n",
        "\n",
        "out2 = app.invoke({\"messages\":[HumanMessage(content=\"Â¿QuÃ© cursos me recomiendas para mi profesiÃ³n?\")]},\n",
        "                  config={\"configurable\":{\"thread_id\": thread_id}})\n",
        "print(out2[\"messages\"][-1].content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIn9wpbm-Tc3"
      },
      "source": [
        "## 5) Estrategia hÃ­brida en LangGraph (Retrieval + Chat)\n",
        "\n",
        "Checkpointing + almacenamiento externo de hechos + retrieval semÃ¡ntico.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahsIKXSQ-Tc3"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from typing import TypedDict, List, Dict, Any\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
        "import json\n",
        "\n",
        "class HybridState(TypedDict):\n",
        "    user_id: str\n",
        "    messages: List[BaseMessage]\n",
        "    facts: Dict[str, Any]\n",
        "    memories: str\n",
        "\n",
        "def retrieval_node(state: HybridState):\n",
        "    uid = state[\"user_id\"]\n",
        "    f = get_facts(uid)\n",
        "    q = next((m.content for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), \"\")\n",
        "    sem = vectordb.similarity_search(q, k=2, filter={\"user_id\": uid})\n",
        "    mem = \"\\n\".join([f\"- {d.page_content}\" for d in sem]) if sem else \"(sin recuerdos)\"\n",
        "    return {\"facts\": f, \"memories\": mem}\n",
        "\n",
        "def chat_node2(state: HybridState):\n",
        "    system = SystemMessage(content=(\n",
        "        \"Usa estos datos para personalizar:\\n\"\n",
        "        f\"Hechos: {json.dumps(state['facts'], ensure_ascii=False)}\\n\"\n",
        "        f\"Memorias:\\n{state['memories']}\\n\"\n",
        "    ))\n",
        "    resp = llm.invoke([system] + state[\"messages\"])\n",
        "    return {\"messages\": state[\"messages\"] + [AIMessage(content=resp.content)]}\n",
        "\n",
        "h = StateGraph(HybridState)\n",
        "h.add_node(\"retrieve\", retrieval_node)\n",
        "h.add_node(\"chat\", chat_node2)\n",
        "h.set_entry_point(\"retrieve\")\n",
        "h.add_edge(\"retrieve\",\"chat\")\n",
        "h.add_edge(\"chat\", END)\n",
        "\n",
        "app2 = h.compile(checkpointer=MemorySaver())\n",
        "\n",
        "print(app2.invoke(\n",
        "    {\"user_id\":\"carlos\",\"messages\":[HumanMessage(content=\"RecomiÃ©ndame cursos para mi profesiÃ³n.\")], \"facts\":{}, \"memories\":\"\"},\n",
        "    config={\"configurable\":{\"thread_id\":\"thread-carlos-2\"}}\n",
        ")[\"messages\"][-1].content)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}