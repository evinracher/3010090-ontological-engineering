{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evinracher/3010090-ontological-engineering/blob/main/week3/part1/3_01_LangGraph_Checkpointing_TimeTravel_Streaming_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "716a4ba2",
      "metadata": {
        "id": "716a4ba2"
      },
      "source": [
        "# LangGraph: Checkpointing, Thread IDs, Time Travel y Streaming (Gemini)\n",
        "Este cuaderno complementa los ejemplos anteriores y cubre **las secciones desde checkpointing en adelante**:\n",
        "- Checkpointing (InMemory/SQLite)\n",
        "- Thread IDs y multi-usuario\n",
        "- Time travel: ver historial, volver a un checkpoint y editar estado\n",
        "- Streaming en LangGraph: `values`, `updates`, `messages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4b19ee6c",
      "metadata": {
        "id": "4b19ee6c",
        "outputId": "96541255-71a4-4955-e33e-6759599b59e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.8)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.9-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-4.2.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.2.13)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.8 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-1.0.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.6)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.63.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic)\n",
            "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.47.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (9.1.4)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.7.3)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.14.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.7)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.2)\n",
            "Downloading langgraph-1.0.9-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-4.2.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.6/463.6 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading langgraph_prebuilt-1.0.8-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: filetype, pydantic-core, pydantic, langchain-google-genai, langgraph-prebuilt, langgraph\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.41.4\n",
            "    Uninstalling pydantic_core-2.41.4:\n",
            "      Successfully uninstalled pydantic_core-2.41.4\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.12.3\n",
            "    Uninstalling pydantic-2.12.3:\n",
            "      Successfully uninstalled pydantic-2.12.3\n",
            "  Attempting uninstall: langgraph-prebuilt\n",
            "    Found existing installation: langgraph-prebuilt 1.0.7\n",
            "    Uninstalling langgraph-prebuilt-1.0.7:\n",
            "      Successfully uninstalled langgraph-prebuilt-1.0.7\n",
            "  Attempting uninstall: langgraph\n",
            "    Found existing installation: langgraph 1.0.8\n",
            "    Uninstalling langgraph-1.0.8:\n",
            "      Successfully uninstalled langgraph-1.0.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.50.0 requires pydantic<=2.12.3,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 langchain-google-genai-4.2.1 langgraph-1.0.9 langgraph-prebuilt-1.0.8 pydantic-2.12.5 pydantic-core-2.41.5\n"
          ]
        }
      ],
      "source": [
        "%pip install -U langgraph langchain-google-genai pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1c60261d",
      "metadata": {
        "id": "1c60261d",
        "outputId": "8c508e25-ccf9-4c8a-ad81-4513f554a02a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key cargada: Sí\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = api_key\n",
        "print('API Key cargada:', 'Sí' if api_key else 'No')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b355f135",
      "metadata": {
        "id": "b355f135",
        "outputId": "f8cc4bfb-ad3b-40a0-9733-0f1058b93776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LLM listo: models/gemini-2.5-flash-lite\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Configure the Gemini API\n",
        "MODEL_ID = os.getenv(\"GEMINI_MODEL\", \"models/gemini-2.5-flash-lite\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=MODEL_ID, temperature=0.2)\n",
        "print(\"✅ LLM listo:\", MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b630e1b",
      "metadata": {
        "id": "3b630e1b"
      },
      "source": [
        "## 1) Checkpointing: guardar el estado automáticamente\n",
        "Un **checkpointer** guarda snapshots del estado del grafo por `thread_id`.\n",
        "- En dev: `InMemorySaver`\n",
        "- Persistencia local: `SqliteSaver`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4aabd096",
      "metadata": {
        "id": "4aabd096",
        "outputId": "82c6f1a3-e848-4bc5-b91d-26aec2aeb82d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7d8b00cbf140>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "class State(TypedDict):\n",
        "    topic: str\n",
        "    joke: str\n",
        "\n",
        "def make_topic(state: State):\n",
        "    return {\"topic\": state[\"topic\"].strip()}\n",
        "\n",
        "def make_joke(state: State):\n",
        "    # Nodo que usa el LLM\n",
        "    prompt = f\"Cuenta un chiste corto sobre: {state['topic']}. Solo una frase.\"\n",
        "    resp = llm.invoke(prompt)\n",
        "    return {\"joke\": resp.content}\n",
        "\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(\"make_topic\", make_topic)\n",
        "builder.add_node(\"make_joke\", make_joke)\n",
        "builder.add_edge(START, \"make_topic\")\n",
        "builder.add_edge(\"make_topic\", \"make_joke\")\n",
        "builder.add_edge(\"make_joke\", END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "42e36892",
      "metadata": {
        "id": "42e36892",
        "outputId": "af6ac45f-d49e-4cbc-ef68-14053457d853",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'gatos',\n",
              " 'joke': '¿Por qué los gatos son malos jugadores de póker? Porque siempre tienen un \"as\" en la manga... ¡y lo usan para rascar!'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Checkpointer en memoria\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "graph = builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"demo-1\"}}\n",
        "\n",
        "out = graph.invoke({\"topic\": \"gatos\"}, config=config)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6b47bac",
      "metadata": {
        "id": "c6b47bac"
      },
      "source": [
        "### Ver el estado guardado (snapshot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "01359f2d",
      "metadata": {
        "id": "01359f2d",
        "outputId": "28393329-a0c2-4b10-94b6-fe050b6a188f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'gatos',\n",
              " 'joke': '¿Por qué los gatos son malos jugadores de póker? Porque siempre tienen un \"as\" en la manga... ¡y lo usan para rascar!'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "snapshot = graph.get_state(config)\n",
        "snapshot.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c256419",
      "metadata": {
        "id": "8c256419"
      },
      "source": [
        "### Ver historial de checkpoints (state history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2362afaa",
      "metadata": {
        "id": "2362afaa",
        "outputId": "73820b3f-2914-42e9-af9e-e5bd477f7ca0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,\n",
              " {'topic': 'gatos',\n",
              "  'joke': '¿Por qué los gatos son malos jugadores de póker? Porque siempre tienen un \"as\" en la manga... ¡y lo usan para rascar!'})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "history = list(graph.get_state_history(config))\n",
        "len(history), history[0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f87de6",
      "metadata": {
        "id": "e7f87de6"
      },
      "source": [
        "## 2) Thread IDs y multi-usuario\n",
        "Cada usuario/sesión debe tener su propio `thread_id` para evitar mezclar estados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3440fdc5",
      "metadata": {
        "id": "3440fdc5",
        "outputId": "ee282007-f0f4-4384-895e-871dd9959d3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carlos: {'topic': 'fútbol', 'joke': '¿Por qué el balón de fútbol fue al psicólogo? Porque tenía muchos problemas de \"golpeo\".'}\n",
            "María : {'topic': 'pizza', 'joke': '¿Por qué la pizza fue a terapia? Porque tenía muchos problemas de *masa*.'}\n"
          ]
        }
      ],
      "source": [
        "config_carlos = {\"configurable\": {\"thread_id\": \"user-carlos\"}}\n",
        "config_maria  = {\"configurable\": {\"thread_id\": \"user-maria\"}}\n",
        "\n",
        "_ = graph.invoke({\"topic\": \"fútbol\"}, config=config_carlos)\n",
        "_ = graph.invoke({\"topic\": \"pizza\"},  config=config_maria)\n",
        "\n",
        "print(\"Carlos:\", graph.get_state(config_carlos).values)\n",
        "print(\"María :\", graph.get_state(config_maria).values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210eca9c",
      "metadata": {
        "id": "210eca9c"
      },
      "source": [
        "## 3) Time travel\n",
        "Con checkpointing activo puedes:\n",
        "- Pedir el historial\n",
        "- Volver a un `checkpoint_id`\n",
        "- Editar el estado con `update_state` (fork)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9d60337d",
      "metadata": {
        "id": "9d60337d",
        "outputId": "49d0f552-c3ca-4c5e-f678-2b5e329b5af2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "latest_id: 1f10f20f-2061-6ab7-8002-7515fb9c2645\n",
            "older_id : 1f10f20f-19fe-6fc8-bfff-09ad100218dd\n"
          ]
        }
      ],
      "source": [
        "# Tomemos el checkpoint más reciente y uno anterior (si existe)\n",
        "history = list(graph.get_state_history(config))\n",
        "latest = history[0]\n",
        "older  = history[-1]\n",
        "\n",
        "latest_id = latest.config[\"configurable\"][\"checkpoint_id\"]\n",
        "older_id  = older.config[\"configurable\"][\"checkpoint_id\"]\n",
        "\n",
        "print(\"latest_id:\", latest_id)\n",
        "print(\"older_id :\", older_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2c5cf0",
      "metadata": {
        "id": "ba2c5cf0"
      },
      "source": [
        "### 3.1 Reproducir desde un checkpoint\n",
        "Si invocas con `checkpoint_id`, LangGraph re-playea lo anterior y continúa desde ahí.\n",
        "\n",
        "LangGraph va generando los ids de los nodos que se han ejecutado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de34059b",
      "metadata": {
        "id": "de34059b"
      },
      "outputs": [],
      "source": [
        "replay_config = {\"configurable\": {\"thread_id\": \"demo-1\", \"checkpoint_id\": older_id}}\n",
        "\n",
        "# Nota: inputs puede ser None si el grafo no requiere nuevas entradas; aquí lo dejamos igual por claridad.\n",
        "replayed = graph.invoke({\"topic\": \"gatos\"}, config=replay_config)\n",
        "replayed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db7cf0c4",
      "metadata": {
        "id": "db7cf0c4"
      },
      "source": [
        "### 3.2 Editar estado (fork) con update_state\n",
        "Ejemplo: cambiamos el topic manualmente y generamos otro chiste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bc1c7af0",
      "metadata": {
        "id": "bc1c7af0",
        "outputId": "18636c2d-a356-4ea6-b47c-86f0d0a59223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'perros',\n",
              " 'joke': '¿Por qué los perros no pueden bailar? Porque no tienen ritmo, ¡solo ladran!'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Actualiza el estado \"como si\" viniera del nodo make_topic\n",
        "graph.update_state(\n",
        "    {\"configurable\": {\"thread_id\": \"demo-1\"}},\n",
        "    {\"topic\": \"perros\"},\n",
        "    as_node=\"make_topic\"\n",
        ")\n",
        "\n",
        "# Ahora corremos desde el estado actual: debería regenerar joke\n",
        "out2 = graph.invoke({\"topic\": \"perros\"}, config={\"configurable\": {\"thread_id\": \"demo-1\"}})\n",
        "out2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf306278",
      "metadata": {
        "id": "cf306278"
      },
      "source": [
        "## 4) Persistencia con SQLite (SqliteSaver)\n",
        "Útil cuando quieres reiniciar el runtime y mantener estado."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -U langgraph langgraph-checkpoint-sqlite"
      ],
      "metadata": {
        "id": "ApDTUJ7xSVly",
        "outputId": "6a7b94ed-cec1-4fb9-c55a-13bcb9dfb58f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ApDTUJ7xSVly",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c071e4c1",
      "metadata": {
        "id": "c071e4c1",
        "outputId": "f4c14363-03ff-43a6-cecc-b2048b8c75f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'topic': 'programación', 'joke': '¿Por qué los programadores prefieren el modo oscuro? Porque la luz atrae a los bugs.'}\n",
            "{'topic': 'programación', 'joke': '¿Por qué los programadores prefieren el modo oscuro? Porque la luz atrae a los bugs.'}\n"
          ]
        }
      ],
      "source": [
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "db_path = \"/content/langgraph_checkpoints.sqlite\"\n",
        "\n",
        "with SqliteSaver.from_conn_string(db_path) as sqlite_cp:\n",
        "    graph_sqlite = builder.compile(checkpointer=sqlite_cp)\n",
        "\n",
        "    cfg = {\"configurable\": {\"thread_id\": \"sqlite-demo\"}}\n",
        "    out = graph_sqlite.invoke({\"topic\": \"programación\"}, config=cfg)\n",
        "    print(out)\n",
        "    print(graph_sqlite.get_state(cfg).values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc59b1b",
      "metadata": {
        "id": "0dc59b1b"
      },
      "source": [
        "## 5) Streaming en LangGraph\n",
        "Modos clave:\n",
        "- `updates`: solo cambios por nodo\n",
        "- `values`: estado completo por paso\n",
        "- `messages`: tokens del LLM (typing effect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a445603c",
      "metadata": {
        "id": "a445603c",
        "outputId": "ebc50502-9d6b-4292-8e25-28b94f014517",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'make_topic': {'topic': 'café'}}\n",
            "{'make_joke': {'joke': '¿Por qué el café fue a terapia? Porque tenía demasiados problemas de \"grano\".'}}\n"
          ]
        }
      ],
      "source": [
        "# 5.1 updates\n",
        "for chunk in graph.stream({\"topic\": \"café\"}, config={\"configurable\": {\"thread_id\": \"stream-1\"}}, stream_mode=\"updates\"):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6449b811",
      "metadata": {
        "id": "6449b811",
        "outputId": "aba570ea-1a6c-46a4-e65b-0f3c8bb1f641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'topic': 'café'}\n",
            "{'topic': 'café'}\n",
            "{'topic': 'café', 'joke': '¿Por qué el café fue a terapia? Porque tenía demasiados problemas de \"grano\".'}\n"
          ]
        }
      ],
      "source": [
        "# 5.2 values\n",
        "for chunk in graph.stream({\"topic\": \"café\"}, config={\"configurable\": {\"thread_id\": \"stream-2\"}}, stream_mode=\"values\"):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4b3f5857",
      "metadata": {
        "id": "4b3f5857",
        "outputId": "063f4e51-154d-4b1a-ae02-d8ee840e5754",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Por qué el café fue a terapia? Porque tenía demasiados problemas de \"grano\"."
          ]
        }
      ],
      "source": [
        "# 5.3 messages (token streaming)\n",
        "for msg_chunk, meta in graph.stream(\n",
        "    {\"topic\": \"café\"},\n",
        "    config={\"configurable\": {\"thread_id\": \"stream-3\"}},\n",
        "    stream_mode=\"messages\",\n",
        "):\n",
        "    # msg_chunk suele ser un AIMessageChunk con .content parcial\n",
        "    if hasattr(msg_chunk, \"content\") and msg_chunk.content:\n",
        "        print(msg_chunk.content, end=\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}