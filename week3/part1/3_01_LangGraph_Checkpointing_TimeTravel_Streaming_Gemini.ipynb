{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evinracher/3010090-ontological-engineering/blob/main/week3/part1/3_01_LangGraph_Checkpointing_TimeTravel_Streaming_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "716a4ba2",
      "metadata": {
        "id": "716a4ba2"
      },
      "source": [
        "# LangGraph: Checkpointing, Thread IDs, Time Travel y Streaming (Gemini)\n",
        "Este cuaderno complementa los ejemplos anteriores y cubre **las secciones desde checkpointing en adelante**:\n",
        "- Checkpointing (InMemory/SQLite)\n",
        "- Thread IDs y multi-usuario\n",
        "- Time travel: ver historial, volver a un checkpoint y editar estado\n",
        "- Streaming en LangGraph: `values`, `updates`, `messages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b19ee6c",
      "metadata": {
        "id": "4b19ee6c"
      },
      "outputs": [],
      "source": [
        "%pip install -U langgraph langchain-google-genai pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c60261d",
      "metadata": {
        "id": "1c60261d"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = api_key\n",
        "print('API Key cargada:', 'Sí' if api_key else 'No')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b355f135",
      "metadata": {
        "id": "b355f135"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Configure the Gemini API\n",
        "MODEL_ID = os.getenv(\"GEMINI_MODEL\", \"models/gemini-2.5-flash-lite\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=MODEL_ID, temperature=0.2)\n",
        "print(\"✅ LLM listo:\", MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b630e1b",
      "metadata": {
        "id": "3b630e1b"
      },
      "source": [
        "## 1) Checkpointing: guardar el estado automáticamente\n",
        "Un **checkpointer** guarda snapshots del estado del grafo por `thread_id`.\n",
        "- En dev: `InMemorySaver`\n",
        "- Persistencia local: `SqliteSaver`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aabd096",
      "metadata": {
        "id": "4aabd096"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "class State(TypedDict):\n",
        "    topic: str\n",
        "    joke: str\n",
        "\n",
        "def make_topic(state: State):\n",
        "    return {\"topic\": state[\"topic\"].strip()}\n",
        "\n",
        "def make_joke(state: State):\n",
        "    # Nodo que usa el LLM\n",
        "    prompt = f\"Cuenta un chiste corto sobre: {state['topic']}. Solo una frase.\"\n",
        "    resp = llm.invoke(prompt)\n",
        "    return {\"joke\": resp.content}\n",
        "\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(\"make_topic\", make_topic)\n",
        "builder.add_node(\"make_joke\", make_joke)\n",
        "builder.add_edge(START, \"make_topic\")\n",
        "builder.add_edge(\"make_topic\", \"make_joke\")\n",
        "builder.add_edge(\"make_joke\", END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42e36892",
      "metadata": {
        "id": "42e36892"
      },
      "outputs": [],
      "source": [
        "# Checkpointer en memoria\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "graph = builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"demo-1\"}}\n",
        "\n",
        "out = graph.invoke({\"topic\": \"gatos\"}, config=config)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6b47bac",
      "metadata": {
        "id": "c6b47bac"
      },
      "source": [
        "### Ver el estado guardado (snapshot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01359f2d",
      "metadata": {
        "id": "01359f2d"
      },
      "outputs": [],
      "source": [
        "snapshot = graph.get_state(config)\n",
        "snapshot.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c256419",
      "metadata": {
        "id": "8c256419"
      },
      "source": [
        "### Ver historial de checkpoints (state history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2362afaa",
      "metadata": {
        "id": "2362afaa"
      },
      "outputs": [],
      "source": [
        "history = list(graph.get_state_history(config))\n",
        "len(history), history[0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f87de6",
      "metadata": {
        "id": "e7f87de6"
      },
      "source": [
        "## 2) Thread IDs y multi-usuario\n",
        "Cada usuario/sesión debe tener su propio `thread_id` para evitar mezclar estados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3440fdc5",
      "metadata": {
        "id": "3440fdc5"
      },
      "outputs": [],
      "source": [
        "config_carlos = {\"configurable\": {\"thread_id\": \"user-carlos\"}}\n",
        "config_maria  = {\"configurable\": {\"thread_id\": \"user-maria\"}}\n",
        "\n",
        "_ = graph.invoke({\"topic\": \"fútbol\"}, config=config_carlos)\n",
        "_ = graph.invoke({\"topic\": \"pizza\"},  config=config_maria)\n",
        "\n",
        "print(\"Carlos:\", graph.get_state(config_carlos).values)\n",
        "print(\"María :\", graph.get_state(config_maria).values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210eca9c",
      "metadata": {
        "id": "210eca9c"
      },
      "source": [
        "## 3) Time travel\n",
        "Con checkpointing activo puedes:\n",
        "- Pedir el historial\n",
        "- Volver a un `checkpoint_id`\n",
        "- Editar el estado con `update_state` (fork)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d60337d",
      "metadata": {
        "id": "9d60337d"
      },
      "outputs": [],
      "source": [
        "# Tomemos el checkpoint más reciente y uno anterior (si existe)\n",
        "history = list(graph.get_state_history(config))\n",
        "latest = history[0]\n",
        "older  = history[-1]\n",
        "\n",
        "latest_id = latest.config[\"configurable\"][\"checkpoint_id\"]\n",
        "older_id  = older.config[\"configurable\"][\"checkpoint_id\"]\n",
        "\n",
        "print(\"latest_id:\", latest_id)\n",
        "print(\"older_id :\", older_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2c5cf0",
      "metadata": {
        "id": "ba2c5cf0"
      },
      "source": [
        "### 3.1 Reproducir desde un checkpoint\n",
        "Si invocas con `checkpoint_id`, LangGraph re-playea lo anterior y continúa desde ahí."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de34059b",
      "metadata": {
        "id": "de34059b"
      },
      "outputs": [],
      "source": [
        "replay_config = {\"configurable\": {\"thread_id\": \"demo-1\", \"checkpoint_id\": older_id}}\n",
        "\n",
        "# Nota: inputs puede ser None si el grafo no requiere nuevas entradas; aquí lo dejamos igual por claridad.\n",
        "replayed = graph.invoke({\"topic\": \"gatos\"}, config=replay_config)\n",
        "replayed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db7cf0c4",
      "metadata": {
        "id": "db7cf0c4"
      },
      "source": [
        "### 3.2 Editar estado (fork) con update_state\n",
        "Ejemplo: cambiamos el topic manualmente y generamos otro chiste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc1c7af0",
      "metadata": {
        "id": "bc1c7af0"
      },
      "outputs": [],
      "source": [
        "# Actualiza el estado \"como si\" viniera del nodo make_topic\n",
        "graph.update_state(\n",
        "    {\"configurable\": {\"thread_id\": \"demo-1\"}},\n",
        "    {\"topic\": \"perros\"},\n",
        "    as_node=\"make_topic\"\n",
        ")\n",
        "\n",
        "# Ahora corremos desde el estado actual: debería regenerar joke\n",
        "out2 = graph.invoke({\"topic\": \"perros\"}, config={\"configurable\": {\"thread_id\": \"demo-1\"}})\n",
        "out2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf306278",
      "metadata": {
        "id": "cf306278"
      },
      "source": [
        "## 4) Persistencia con SQLite (SqliteSaver)\n",
        "Útil cuando quieres reiniciar el runtime y mantener estado."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -U langgraph langgraph-checkpoint-sqlite"
      ],
      "metadata": {
        "id": "ApDTUJ7xSVly"
      },
      "id": "ApDTUJ7xSVly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c071e4c1",
      "metadata": {
        "id": "c071e4c1"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "db_path = \"/content/langgraph_checkpoints.sqlite\"\n",
        "\n",
        "with SqliteSaver.from_conn_string(db_path) as sqlite_cp:\n",
        "    graph_sqlite = builder.compile(checkpointer=sqlite_cp)\n",
        "\n",
        "    cfg = {\"configurable\": {\"thread_id\": \"sqlite-demo\"}}\n",
        "    out = graph_sqlite.invoke({\"topic\": \"programación\"}, config=cfg)\n",
        "    print(out)\n",
        "    print(graph_sqlite.get_state(cfg).values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc59b1b",
      "metadata": {
        "id": "0dc59b1b"
      },
      "source": [
        "## 5) Streaming en LangGraph\n",
        "Modos clave:\n",
        "- `updates`: solo cambios por nodo\n",
        "- `values`: estado completo por paso\n",
        "- `messages`: tokens del LLM (typing effect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a445603c",
      "metadata": {
        "id": "a445603c"
      },
      "outputs": [],
      "source": [
        "# 5.1 updates\n",
        "for chunk in graph.stream({\"topic\": \"café\"}, config={\"configurable\": {\"thread_id\": \"stream-1\"}}, stream_mode=\"updates\"):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6449b811",
      "metadata": {
        "id": "6449b811"
      },
      "outputs": [],
      "source": [
        "# 5.2 values\n",
        "for chunk in graph.stream({\"topic\": \"café\"}, config={\"configurable\": {\"thread_id\": \"stream-2\"}}, stream_mode=\"values\"):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b3f5857",
      "metadata": {
        "id": "4b3f5857"
      },
      "outputs": [],
      "source": [
        "# 5.3 messages (token streaming)\n",
        "for msg_chunk, meta in graph.stream(\n",
        "    {\"topic\": \"café\"},\n",
        "    config={\"configurable\": {\"thread_id\": \"stream-3\"}},\n",
        "    stream_mode=\"messages\",\n",
        "):\n",
        "    # msg_chunk suele ser un AIMessageChunk con .content parcial\n",
        "    if hasattr(msg_chunk, \"content\") and msg_chunk.content:\n",
        "        print(msg_chunk.content, end=\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}