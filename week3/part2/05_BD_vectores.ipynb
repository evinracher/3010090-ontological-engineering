{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evinracher/3010090-ontological-engineering/blob/main/week3/part2/05_BD_vectores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62baa5ee",
      "metadata": {
        "id": "62baa5ee"
      },
      "source": [
        "# 01 - Cargadores de documentos con LangChain 1.0 y Gemini üá®üá¥\n",
        "\n",
        "En este cuaderno trabajaremos el tema de **conectores de datos** en el contexto de **LangChain 1.0** usando como LLM **Gemini**.\n",
        "\n",
        "Ver√°s los siguientes temas b√°sicos, listos para ejecutar en **Google Colab**:\n",
        "\n",
        "1. Instalaci√≥n de dependencias para LangChain 1.0 + Gemini  \n",
        "2. Configuraci√≥n de la API Key de Gemini  \n",
        "3. Cargadores de documentos (PDF como caso base)  \n",
        "4. Transformaci√≥n de documentos: divisi√≥n en *chunks*  \n",
        "5. Creaci√≥n de *embeddings* con Gemini  \n",
        "6. Creaci√≥n de una base vectorial (FAISS)  \n",
        "7. Cadena de pregunta-respuesta (RAG) sobre el PDF usando Gemini  \n",
        "\n",
        "> **Nota:** Todos los ejemplos est√°n simplificados para que sirvan como plantilla did√°ctica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15258831",
      "metadata": {
        "id": "15258831"
      },
      "source": [
        "## 1. Instalaci√≥n de librer√≠as necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b95ddb8",
      "metadata": {
        "id": "3b95ddb8"
      },
      "outputs": [],
      "source": [
        "# Si est√°s en Google Colab, ejecuta esta celda primero.\n",
        "# Puede tardar un poco.\n",
        "\n",
        "!pip -q install -U langchain-core langchain-community langchain-text-splitters google-generativeai langchain-google-genai fastembed faiss-cpu pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63dc2732",
      "metadata": {
        "id": "63dc2732"
      },
      "source": [
        "## 2. Configuraci√≥n de la clave de API de Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd2b0a9b",
      "metadata": {
        "id": "dd2b0a9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981a4266-9961-446d-dd3e-9ecefb71514c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key cargada: S√≠\n",
            "Primeros caracteres: AIzaSyBxr2\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Obtener la API key desde userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Opcional: Guardarla como variable de entorno\n",
        "os.environ['GOOGLE_API_KEY'] = api_key\n",
        "\n",
        "# Verificar que se haya cargado correctamente\n",
        "print(\"API Key cargada:\", \"S√≠\" if api_key else \"No\")\n",
        "print(\"Primeros caracteres:\", api_key[:10] if api_key else \"No encontrada\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e0a866e",
      "metadata": {
        "id": "0e0a866e"
      },
      "source": [
        "## 3. Importaciones base: LangChain 1.0 + Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6fbde7",
      "metadata": {
        "id": "aa6fbde7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d5a1b2-e72b-4352-b5b0-6c8be3cf75cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Librer√≠as importadas correctamente.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4be4efb6",
      "metadata": {
        "id": "4be4efb6"
      },
      "source": [
        "## 4. Cargadores de documentos: ejemplo con PDF\n",
        "\n",
        "En este ejemplo vamos a:\n",
        "\n",
        "1. Subir un archivo PDF a Colab  \n",
        "2. Cargarlo con `PyPDFLoader`  \n",
        "3. Ver parte de su contenido\n",
        "\n",
        "> Puedes usar cualquier PDF que tengas a mano (art√≠culo, paper, cap√≠tulo de libro, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "836d8d2a",
      "metadata": {
        "id": "836d8d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "176a21a8-e76f-4e94-e8f6-47a2974f31bb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d7c59fab-9bb2-4f7e-b2c2-48c0ae0253a1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d7c59fab-9bb2-4f7e-b2c2-48c0ae0253a1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Documento tecnolog√≠as emergentes.pdf to Documento tecnolog√≠as emergentes (1).pdf\n",
            "üìÑ Archivo cargado: Documento tecnolog√≠as emergentes (1).pdf\n"
          ]
        }
      ],
      "source": [
        "# 4.1. Subir el archivo PDF desde tu equipo\n",
        "# Ejecuta esta celda y selecciona un archivo .pdf\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "upload = files.upload()\n",
        "pdf_path = list(upload.keys())[0]\n",
        "print(f\"üìÑ Archivo cargado: {pdf_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26739f62",
      "metadata": {
        "id": "26739f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6f99f9-6ab0-4c2f-9f0a-c6d141649bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ P√°ginas cargadas: 4\n",
            "\n",
            "üìÑ Vista previa del contenido de la primera p√°gina:\n",
            "\n",
            "Estas son las 9 tecnolog√≠as \n",
            "emergentes para el pr√≥ximo \n",
            "2025 \n",
            "  \n",
            "‚ÄúQue la tecnolog√≠a ha cambiado nuestra manera de vivir e interactuar \n",
            "es un hecho. Sin embargo, a√∫n no somos conscientes de las \n",
            "potencialidades de usos de las tecnolog√≠as.Por ejemplo, para el a√±o \n",
            "2025 se espera una verdadera revoluci√≥n tecnol√≥gica, sobre todo \n",
            "enfocado en el sector bio-m√©dico pero tambi√©n en las relaciones \n",
            "humanas entre individuos a distancia, en la protecci√≥n del medio \n",
            "ambiente o en la protecci√≥n de nuestros datos personales‚Äù, afirma \n",
            "Juan Quintanilla, director general de Syntonize. \n",
            "9 Tecnolog√≠as emergentes seg√∫n Syntonize \n",
            "La aplicaci√≥n de nuevas tecnolog√≠as que hagan m√°s f√°cil la vida a \n",
            "profesionales, estudiantes, mayores, empresas o instituciones \n",
            "p√∫blicas se espera que aumente en los pr√≥ximos a√±os. Entre ellas se \n",
            "encuentran; \n",
            "ÔÇ∑ Producci√≥n optimizada por la Inteligencia Artificial: las \n",
            "empresas est√°n adoptando r√°pidamente tecnolog√≠as basadas \n",
            "en la nube. Gracias a ello, podr√°n agregar, transf\n"
          ]
        }
      ],
      "source": [
        "# 4.2. Cargar el PDF usando PyPDFLoader\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"‚úÖ P√°ginas cargadas: {len(docs)}\")\n",
        "print(\"\\nüìÑ Vista previa del contenido de la primera p√°gina:\\n\")\n",
        "print(docs[0].page_content[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76db347d",
      "metadata": {
        "id": "76db347d"
      },
      "source": [
        "## 5. Transformaci√≥n de documentos: divisi√≥n en *chunks*\n",
        "\n",
        "Para usar un LLM de forma eficiente y tambi√©n para crear una base vectorial,\n",
        "es conveniente dividir el documento en fragmentos (*chunks*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feeb9577",
      "metadata": {
        "id": "feeb9577",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e2ce581-d65e-412d-c73b-1ac48b0c861a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ N√∫mero de chunks generados: 9\n",
            "\n",
            "üìÑ Ejemplo de un chunk:\n",
            "\n",
            "Estas son las 9 tecnolog√≠as \n",
            "emergentes para el pr√≥ximo \n",
            "2025 \n",
            "  \n",
            "‚ÄúQue la tecnolog√≠a ha cambiado nuestra manera de vivir e interactuar \n",
            "es un hecho. Sin embargo, a√∫n no somos conscientes de las \n",
            "potencialidades de usos de las tecnolog√≠as.Por ejemplo, para el a√±o \n",
            "2025 se espera una verdadera revoluci√≥n tecnol√≥gica, sobre todo \n",
            "enfocado en el sector bio-m√©dico pero tambi√©n en las relaciones \n",
            "humanas entre individuos a distancia, en la protecci√≥n del medio \n",
            "ambiente o en la protecci√≥n de nuestros \n"
          ]
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,      # longitud m√°xima de cada chunk\n",
        "    chunk_overlap=100,   # solapamiento entre chunks para mantener contexto\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"‚úÖ N√∫mero de chunks generados: {len(chunks)}\")\n",
        "print(\"\\nüìÑ Ejemplo de un chunk:\\n\")\n",
        "print(chunks[0].page_content[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d76eaf46",
      "metadata": {
        "id": "d76eaf46"
      },
      "source": [
        "## 6. Incrustaciones de texto (Embeddings) con Gemini\n",
        "\n",
        "Usaremos el modelo `BAAI/bge-small-en-v1.5` de FastEmbedEmbeddings para convertir cada chunk\n",
        "en un vector num√©rico de alta dimensi√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "321df170",
      "metadata": {
        "id": "321df170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386e2be6-62cd-4022-cf36-df5bb991e33a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensi√≥n del vector de ejemplo: 384\n"
          ]
        }
      ],
      "source": [
        "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Probar con un texto simple\n",
        "vector_ejemplo = embeddings.embed_query(\"La inteligencia artificial est√° transformando la educaci√≥n.\")\n",
        "print(f\"Dimensi√≥n del vector de ejemplo: {len(vector_ejemplo)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86496cac",
      "metadata": {
        "id": "86496cac"
      },
      "source": [
        "## 7. Creaci√≥n de una base vectorial con FAISS\n",
        "\n",
        "Ahora creamos una base de datos vectorial a partir de los *chunks* usando FAISS.\n",
        "Esto permitir√° hacer **b√∫squeda sem√°ntica** sobre el contenido del PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4375b5",
      "metadata": {
        "id": "4d4375b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdaceffa-3884-4b2e-f299-b4762061b9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Base vectorial creada y retriever configurado.\n"
          ]
        }
      ],
      "source": [
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "print(\"‚úÖ Base vectorial creada y retriever configurado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba373b4",
      "metadata": {
        "id": "0ba373b4"
      },
      "source": [
        "## 8. Definir el modelo de chat Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e6830b",
      "metadata": {
        "id": "84e6830b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc5a6699-4abf-4732-a897-0d384d3320ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Modelo Gemini configurado: models/gemini-2.5-flash-lite\n"
          ]
        }
      ],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"models/gemini-2.5-flash-lite\",\n",
        "    temperature=0.3,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo Gemini configurado: models/gemini-2.5-flash-lite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5c7f768",
      "metadata": {
        "id": "e5c7f768"
      },
      "source": [
        "## 9. Cadena de Pregunta-Respuesta (RAG) sobre el PDF\n",
        "\n",
        "Usaremos el patr√≥n LCEL de LangChain 1.0 para construir una cadena que:\n",
        "\n",
        "1. Recupera los chunks m√°s relevantes v√≠a `retriever`  \n",
        "2. Forma un prompt con contexto + pregunta del usuario  \n",
        "3. Env√≠a el prompt a Gemini  \n",
        "4. Devuelve una respuesta en lenguaje natural\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d66e5ce",
      "metadata": {
        "id": "4d66e5ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702a2f3d-fada-4317-f3c6-2a72fc61e98c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cadena RAG construida correctamente.\n"
          ]
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Eres un asistente experto en comprensi√≥n de documentos.\n",
        "\n",
        "Usando exclusivamente la siguiente informaci√≥n de contexto, responde de forma clara,\n",
        "estructurada y en espa√±ol a la pregunta del usuario.\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pregunta:\n",
        "{question}\n",
        "\"\"\")\n",
        "\n",
        "# 1) Definimos un paso que obtiene contexto desde el retriever\n",
        "def get_context(query: str):\n",
        "    docs_relacionados = retriever.invoke(query)   # << CORRECTO EN LC 1.0\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs_relacionados])\n",
        "\n",
        "\n",
        "# 2) Construimos la cadena con LCEL\n",
        "rag_chain = (\n",
        "    RunnableParallel(\n",
        "        context=lambda x: get_context(x[\"question\"]),\n",
        "        question=RunnablePassthrough()\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Cadena RAG construida correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4493caa7",
      "metadata": {
        "id": "4493caa7"
      },
      "source": [
        "## 10. Probar la cadena RAG con tus propias preguntas\n",
        "\n",
        "Ahora puedes hacer preguntas en lenguaje natural sobre el contenido del PDF cargado.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta = \"¬øCu√°l es el tema principal del documento y cu√°les son sus conclusiones m√°s importantes?\"\n",
        "\n",
        "respuesta = rag_chain.invoke({\"question\": pregunta})\n",
        "\n",
        "print(\"üß† Pregunta:\", pregunta)\n",
        "print(\"\\nüìå Respuesta de Gemini:\\n\")\n",
        "print(respuesta)"
      ],
      "metadata": {
        "id": "8hEbat6Kk5ug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757ac064-787e-4bb2-f6d0-d3ac6be8dc82"
      },
      "id": "8hEbat6Kk5ug",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Pregunta: ¬øCu√°l es el tema principal del documento y cu√°les son sus conclusiones m√°s importantes?\n",
            "\n",
            "üìå Respuesta de Gemini:\n",
            "\n",
            "El documento aborda diversas tendencias tecnol√≥gicas y su impacto futuro en diferentes √°mbitos. Las conclusiones m√°s importantes son:\n",
            "\n",
            "*   **Microbiomas y pat√≥genos:** La tecnolog√≠a acelerar√° la capacidad de muestrear, digitalizar e interpretar datos de microbiomas, lo que transformar√° la comprensi√≥n de la propagaci√≥n de pat√≥genos.\n",
            "*   **Privacidad:** La privacidad ser√° generalizada y priorizada, con tecnolog√≠as de mejora de la privacidad convirti√©ndose en una categor√≠a tecnol√≥gica propia y un elemento fundamental en las estrategias empresariales.\n",
            "*   **Aplicaci√≥n de nuevas tecnolog√≠as:** Se espera un aumento en la aplicaci√≥n de nuevas tecnolog√≠as que faciliten la vida de profesionales, estudiantes, mayores, empresas e instituciones p√∫blicas.\n",
            "*   **5G:** La tecnolog√≠a 5G mejorar√° la econom√≠a global y salvar√° vidas al resolver problemas de confiabilidad de red, permitiendo servicios de alta capacidad como telesalud, telecirug√≠a y servicios de emergencia.\n",
            "*   **C√°ncer:** La tecnolog√≠a impulsar√° un cambio en el manejo del c√°ncer, trat√°ndolo como una afecci√≥n cr√≥nica, permitiendo una identificaci√≥n precisa y un tratamiento revolucionario.\n",
            "*   **Ruptura de la barrera virtual-real:** Se acelerar√° la conexi√≥n entre personas a nivel humano a trav√©s de inteligencia artificial, difuminando la l√≠nea entre el espacio f√≠sico y virtual.\n",
            "*   **Cambio clim√°tico:** Se ampliar√°n las tecnolog√≠as de emisi√≥n negativa, como la eliminaci√≥n de di√≥xido de carbono, para limitar el calentamiento global y eliminar el CO2 hist√≥rico del aire.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al hacer una pregunta m√°s espec√≠fica:"
      ],
      "metadata": {
        "id": "WqA-92qa40Pw"
      },
      "id": "WqA-92qa40Pw"
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta = \"¬øA qu√© se refieren con 'La privacidad estar√° generalizada y priorizada'\"\n",
        "\n",
        "respuesta = rag_chain.invoke({\"question\": pregunta})\n",
        "\n",
        "print(\"üß† Pregunta:\", pregunta)\n",
        "print(\"\\nüìå Respuesta de Gemini:\\n\")\n",
        "print(respuesta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZiToqme4jri",
        "outputId": "71fd1c72-28e0-400d-be7e-e279087ab962"
      },
      "id": "TZiToqme4jri",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Pregunta: ¬øA qu√© se refieren con 'La privacidad estar√° generalizada y priorizada'\n",
            "\n",
            "üìå Respuesta de Gemini:\n",
            "\n",
            "Con \"La privacidad estar√° generalizada y priorizada\" se refieren a que la capacidad de los consumidores para proteger y controlar sus activos de datos confidenciales se convertir√° en la norma, no en la excepci√≥n. Las tecnolog√≠as de mejora de la privacidad ser√°n una categor√≠a tecnol√≥gica independiente y un componente esencial de las estrategias de privacidad y seguridad de las empresas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3b3eff7",
      "metadata": {
        "id": "c3b3eff7"
      },
      "source": [
        "---\n",
        "\n",
        "### ‚úÖ Resumen de este cuaderno\n",
        "\n",
        "En este notebook vimos:\n",
        "\n",
        "- C√≥mo configurar **Gemini** en Google Colab usando `langchain-google-genai`.  \n",
        "- C√≥mo cargar un **PDF** con `PyPDFLoader`.  \n",
        "- C√≥mo dividir el documento en **chunks** con `RecursiveCharacterTextSplitter`.  \n",
        "- C√≥mo generar **embeddings** con `GoogleGenerativeAIEmbeddings`.  \n",
        "- C√≥mo construir una base vectorial con **FAISS**.  \n",
        "- C√≥mo implementar una cadena **RAG** con LCEL para hacer preguntas sobre el PDF.\n",
        "\n",
        "Este cuaderno sirve como plantilla base para extensiones posteriores: integraci√≥n con m√°s tipos de documentos,\n",
        "uso de otros almacenes vectoriales, o construcci√≥n de interfaces (por ejemplo, Gradio o Streamlit).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}