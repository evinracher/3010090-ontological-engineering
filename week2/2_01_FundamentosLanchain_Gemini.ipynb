{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evinracher/3010090-ontological-engineering/blob/main/week2/2_01_FundamentosLanchain_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHKeykj1v6p2"
      },
      "source": [
        "# üß† LangChain v1.0 con Gemini 2.5 Pro\n",
        "## PromptTemplate, ChatModel y Chains\n",
        "Autor: Profesor Jaime Guzm√°n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Obtener la API key desde userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Opcional: Guardarla como variable de entorno\n",
        "os.environ['GOOGLE_API_KEY'] = api_key\n",
        "\n",
        "# Verificar que se haya cargado correctamente\n",
        "print(\"API Key cargada:\", \"S√≠\" if api_key else \"No\")\n",
        "print(\"Primeros caracteres:\", api_key[:10] if api_key else \"No encontrada\")\n"
      ],
      "metadata": {
        "id": "9W6XFv54wkVK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7122c8f1-4697-4f8b-e4a5-551c5e3ab60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key cargada: S√≠\n",
            "Primeros caracteres: AIzaSyBxr2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9bGESv3v6p3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "491cfebb-afe5-4286-c50d-12b194c0f59c"
      },
      "source": [
        "# =============================================================\n",
        "# üü¶ 1. Instalaci√≥n y Configuraci√≥n Inicial\n",
        "# =============================================================\n",
        "!pip install \"langchain>=1,<2\" langchain-core langchain-google-genai"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain<2,>=1 in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.9)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-4.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langchain<2,>=1) (1.0.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2,>=1) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.6.9)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.14.0)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.62.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.47.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.32.4)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2,>=1) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2,>=1) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2,>=1) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2,>=1) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2,>=1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2,>=1) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2,>=1) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain<2,>=1) (1.12.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.2)\n",
            "Downloading langchain_google_genai-4.2.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# üü® 2. PromptTemplate: Plantillas din√°micas\n",
        "# =============================================================\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Crear plantilla con variable\n",
        "plantilla_traduccion = PromptTemplate.from_template(\n",
        "    \"\"\"Traduce al espa√±ol el siguiente texto:\n",
        "\n",
        "{texto}\"\"\"\n",
        ")\n",
        "\n",
        "# Usar la plantilla\n",
        "prompt_generado = plantilla_traduccion.format(\n",
        "    texto='Artificial intelligence is transforming education.'\n",
        ")\n",
        "print(prompt_generado)"
      ],
      "metadata": {
        "id": "F5OAYjc4yNTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493cd798-b2b3-4898-d680-c25cded9eb21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traduce al espa√±ol el siguiente texto:\n",
            "\n",
            "Artificial intelligence is transforming education.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# üü© 3. ChatModel: Comunicaci√≥n con gemini-2.5-flash-lite\n",
        "# =============================================================\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage  # Importar desde langchain_core\n",
        "\n",
        "#chat = ChatGoogleGenerativeAI(model='gemini-2.5-flash-exp', temperature=0.7)\n",
        "chat = ChatGoogleGenerativeAI(model='gemini-2.5-flash-lite', temperature=0.7, google_api_key=api_key)\n",
        "\n",
        "\n",
        "respuesta = chat.invoke([\n",
        "    HumanMessage(content='Explica de forma sencilla qu√© es el aprendizaje profundo.')\n",
        "])\n",
        "print('\\nüß† Respuesta del modelo:\\n', respuesta.content)"
      ],
      "metadata": {
        "id": "BjKZZArxzxCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b9230e-3997-4678-b3c4-dc48f2701b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Respuesta del modelo:\n",
            " Imagina que quieres ense√±ar a una computadora a reconocer perros en fotos.\n",
            "\n",
            "**Aprendizaje Profundo (Deep Learning) es como entrenar a una red neuronal artificial, inspirada en c√≥mo funciona nuestro cerebro, con much√≠simos ejemplos.**\n",
            "\n",
            "Aqu√≠ te lo explico de forma sencilla:\n",
            "\n",
            "*   **Como un beb√© aprendiendo:** Piensa en c√≥mo un beb√© aprende. No le explicas las reglas de \"un perro tiene cuatro patas, cola y orejas\". Simplemente le muestras muchas fotos de perros y le dices \"esto es un perro\". Con el tiempo, el beb√© empieza a notar patrones por s√≠ mismo: la forma del hocico, el tipo de pelo, la silueta.\n",
            "\n",
            "*   **Capas y capas de \"neuronas\":** El aprendizaje profundo utiliza algo llamado **redes neuronales artificiales**. Estas redes est√°n formadas por muchas capas de \"neuronas\" interconectadas, como si fueran peque√±os procesadores que trabajan juntos.\n",
            "\n",
            "    *   Las **primeras capas** se encargan de detectar cosas muy b√°sicas: bordes, l√≠neas, colores.\n",
            "    *   Las **capas intermedias** empiezan a combinar estas caracter√≠sticas b√°sicas para reconocer formas m√°s complejas: un ojo, una oreja, una rueda de un coche.\n",
            "    *   Las **√∫ltimas capas** juntan todas estas formas para identificar el objeto completo: ¬°un perro!\n",
            "\n",
            "*   **\"Profundo\" por las capas:** Se llama \"profundo\" porque estas redes neuronales tienen **muchas capas**. Cuantas m√°s capas, m√°s compleja puede ser la informaci√≥n que la red puede aprender a procesar.\n",
            "\n",
            "*   **Aprende de la experiencia (datos):** La clave es que la red aprende **autom√°ticamente** de una gran cantidad de datos. Le das miles o millones de fotos de perros (y tambi√©n fotos de cosas que no son perros), y la red va ajustando sus conexiones internas para mejorar su capacidad de distinguir un perro de cualquier otra cosa.\n",
            "\n",
            "**En resumen:**\n",
            "\n",
            "El aprendizaje profundo es una forma de ense√±ar a las computadoras a aprender de una manera similar a como lo hacen los humanos, usando redes neuronales con muchas capas que procesan informaci√≥n de forma jer√°rquica, a partir de grandes cantidades de datos.\n",
            "\n",
            "**¬øPara qu√© sirve?**\n",
            "\n",
            "Se usa para cosas incre√≠bles como:\n",
            "\n",
            "*   Reconocer caras en fotos.\n",
            "*   Traducir idiomas autom√°ticamente.\n",
            "*   Conducir coches aut√≥nomos.\n",
            "*   Crear m√∫sica o arte.\n",
            "*   Diagnosticar enfermedades.\n",
            "\n",
            "Es como darle a las computadoras la capacidad de \"ver\", \"o√≠r\" y \"entender\" el mundo de una forma mucho m√°s avanzada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# üüß 4. Chains: Uniendo Prompt + Modelo (Forma moderna LCEL)\n",
        "# =============================================================\n",
        "\n",
        "# Usar el operador | para encadenar (pipe operator)\n",
        "cadena_traduccion = plantilla_traduccion | chat\n",
        "\n",
        "# Invocar\n",
        "resultado = cadena_traduccion.invoke({'texto': 'The quick brown fox jumps over the lazy dog.'})\n",
        "print('\\nüåç Traducci√≥n generada por gemini-2.5-flash-lite:\\n', resultado.content)"
      ],
      "metadata": {
        "id": "CBg7NvXu4tQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998b3a73-79a6-4243-ea28-98a8f8d8a1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üåç Traducci√≥n generada por gemini-2.5-flash-lite:\n",
            " La traducci√≥n al espa√±ol es:\n",
            "\n",
            "El r√°pido zorro marr√≥n salta sobre el perro perezoso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfUTyJPlv6p5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0e0f74-d011-4f2f-f16a-1dc88f195289"
      },
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "# 1Ô∏è‚É£ Definir el modelo\n",
        "chat = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
        "\n",
        "# 2Ô∏è‚É£ Definir la plantilla\n",
        "plantilla_traduccion = ChatPromptTemplate.from_template(\"Traduce al espa√±ol el siguiente texto: {texto}\")\n",
        "\n",
        "# 3Ô∏è‚É£ Crear la cadena moderna\n",
        "cadena_traduccion = RunnableSequence(plantilla_traduccion | chat)\n",
        "\n",
        "# 4Ô∏è‚É£ Invocar la cadena\n",
        "resultado = cadena_traduccion.invoke({\"texto\": \"The quick brown fox jumps over the lazy dog.\"})\n",
        "print(\"üåç Traducci√≥n generada por Gemini:\", resultado.content)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåç Traducci√≥n generada por Gemini: La traducci√≥n al espa√±ol de \"The quick brown fox jumps over the lazy dog\" es:\n",
            "\n",
            "**El r√°pido zorro marr√≥n salta sobre el perro perezoso.**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH-ZX_t2v6p6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8110e9-ef4c-464a-cddb-354d56731f1b"
      },
      "source": [
        "# =============================================================\n",
        "# üü™ 5. Ejercicio Integrador (versi√≥n moderna)\n",
        "# =============================================================\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "# 1Ô∏è‚É£ Definir el modelo (aseg√∫rate de tener la clave configurada)\n",
        "chat = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
        "\n",
        "# 2Ô∏è‚É£ Definir la plantilla de prompt\n",
        "plantilla_tema = ChatPromptTemplate.from_template('''Explica el concepto de {tema} en tres partes:\n",
        "1. Una explicaci√≥n breve.\n",
        "2. Un ejemplo pr√°ctico en Python.\n",
        "3. Una pregunta reflexiva final.''')\n",
        "\n",
        "# 3Ô∏è‚É£ Crear la \"cadena\" moderna con RunnableSequence\n",
        "cadena_tema = RunnableSequence(plantilla_tema | chat)\n",
        "\n",
        "# 4Ô∏è‚É£ Ejecutar el pipeline\n",
        "salida = cadena_tema.invoke({\"tema\": \"machine learning\"})\n",
        "\n",
        "# 5Ô∏è‚É£ Mostrar el resultado\n",
        "print(\"üìò Resultado del ejercicio integrador:\\n\")\n",
        "print(salida.content)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìò Resultado del ejercicio integrador:\n",
            "\n",
            "¬°Claro! Aqu√≠ tienes una explicaci√≥n del Machine Learning dividida en tres partes:\n",
            "\n",
            "## Machine Learning: Aprendiendo de los Datos\n",
            "\n",
            "### 1. Explicaci√≥n Breve\n",
            "\n",
            "El Machine Learning (ML), o Aprendizaje Autom√°tico, es una rama de la Inteligencia Artificial que permite a los sistemas inform√°ticos **aprender de datos sin ser expl√≠citamente programados**. En lugar de seguir un conjunto de instrucciones fijas para realizar una tarea, los algoritmos de ML utilizan **patrones y relaciones** encontrados en grandes cantidades de datos para **tomar decisiones o hacer predicciones**. Es como ense√±ar a un ni√±o a reconocer un perro mostr√°ndole muchas fotos de perros, en lugar de describirle detalladamente c√≥mo es un perro. El sistema \"aprende\" a identificar las caracter√≠sticas comunes que definen a un perro.\n",
            "\n",
            "### 2. Ejemplo Pr√°ctico en Python\n",
            "\n",
            "Imaginemos que queremos construir un modelo que prediga si un correo electr√≥nico es spam o no. Utilizaremos un modelo simple de clasificaci√≥n lineal.\n",
            "\n",
            "```python\n",
            "# Primero, necesitamos algunas librer√≠as\n",
            "from sklearn.feature_extraction.text import CountVectorizer\n",
            "from sklearn.naive_bayes import MultinomialNB\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import accuracy_score\n",
            "\n",
            "# --- 1. Datos de ejemplo ---\n",
            "# Emails de ejemplo (simplificados) y sus etiquetas (0: no spam, 1: spam)\n",
            "emails = [\n",
            "    \"Oferta especial, compra ahora\",\n",
            "    \"Reuni√≥n importante ma√±ana\",\n",
            "    \"Gana dinero r√°pido\",\n",
            "    \"Tu pedido ha sido confirmado\",\n",
            "    \"Descuento exclusivo, no te lo pierdas\",\n",
            "    \"Hola, ¬øc√≥mo est√°s?\",\n",
            "    \"Notificaci√≥n de env√≠o\",\n",
            "    \"Felicidades, has ganado un premio\",\n",
            "    \"Recordatorio de cita\",\n",
            "    \"Informaci√≥n sobre tu suscripci√≥n\"\n",
            "]\n",
            "etiquetas = [1, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n",
            "\n",
            "# --- 2. Preprocesamiento de datos ---\n",
            "# Convertir texto a vectores num√©ricos\n",
            "# CountVectorizer crea un vocabulario y cuenta la frecuencia de cada palabra en cada email.\n",
            "vectorizador = CountVectorizer()\n",
            "X = vectorizador.fit_transform(emails) # X son los datos de entrada (vectores)\n",
            "\n",
            "# --- 3. Dividir los datos ---\n",
            "# Dividimos los datos en un conjunto de entrenamiento y un conjunto de prueba.\n",
            "# El modelo aprende de los datos de entrenamiento y se eval√∫a con los datos de prueba.\n",
            "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, etiquetas, test_size=0.3, random_state=42)\n",
            "\n",
            "# --- 4. Elegir y entrenar el modelo ---\n",
            "# Usamos un clasificador Naive Bayes Multinomial, com√∫n para clasificaci√≥n de texto.\n",
            "modelo = MultinomialNB()\n",
            "modelo.fit(X_entrenamiento, y_entrenamiento) # Entrenamos el modelo\n",
            "\n",
            "# --- 5. Hacer predicciones ---\n",
            "predicciones = modelo.predict(X_prueba)\n",
            "\n",
            "# --- 6. Evaluar el modelo ---\n",
            "precision = accuracy_score(y_prueba, predicciones)\n",
            "print(f\"La precisi√≥n del modelo es: {precision:.2f}\")\n",
            "\n",
            "# --- 7. Usar el modelo para predecir un nuevo email ---\n",
            "nuevo_email = [\"Oferta incre√≠ble, gana dinero f√°cil\"]\n",
            "nuevo_email_vectorizado = vectorizador.transform(nuevo_email)\n",
            "prediccion_nuevo = modelo.predict(nuevo_email_vectorizado)\n",
            "\n",
            "if prediccion_nuevo[0] == 1:\n",
            "    print(f\"El nuevo email '{nuevo_email[0]}' ha sido clasificado como: SPAM\")\n",
            "else:\n",
            "    print(f\"El nuevo email '{nuevo_email[0]}' ha sido clasificado como: NO SPAM\")\n",
            "```\n",
            "\n",
            "**Explicaci√≥n del c√≥digo:**\n",
            "\n",
            "1.  **Datos de ejemplo:** Creamos listas de correos electr√≥nicos y sus etiquetas correspondientes (si son spam o no).\n",
            "2.  **Preprocesamiento:** `CountVectorizer` transforma el texto en n√∫meros que el modelo puede entender. Cada email se convierte en un vector donde cada elemento representa la frecuencia de una palabra del vocabulario general.\n",
            "3.  **Dividir datos:** Separamos los datos en dos grupos: uno para que el modelo aprenda (`entrenamiento`) y otro para probar qu√© tan bien ha aprendido (`prueba`).\n",
            "4.  **Entrenar modelo:** `MultinomialNB()` es un algoritmo de Machine Learning. `modelo.fit()` utiliza los datos de entrenamiento para que el modelo aprenda los patrones que distinguen el spam del no spam.\n",
            "5.  **Predecir:** `modelo.predict()` utiliza el modelo entrenado para predecir la etiqueta de los correos electr√≥nicos en el conjunto de prueba.\n",
            "6.  **Evaluar:** `accuracy_score()` calcula qu√© porcentaje de las predicciones del modelo fueron correctas.\n",
            "7.  **Nuevo email:** Mostramos c√≥mo usar el modelo entrenado para clasificar un correo electr√≥nico completamente nuevo.\n",
            "\n",
            "### 3. Pregunta Reflexiva Final\n",
            "\n",
            "Considerando la creciente capacidad de los modelos de Machine Learning para generar contenido (texto, im√°genes, m√∫sica) y tomar decisiones cada vez m√°s complejas, **¬øcu√°les crees que son los desaf√≠os √©ticos m√°s importantes que debemos abordar a medida que la inteligencia artificial se integra m√°s profundamente en nuestra sociedad, y c√≥mo podemos mitigarlos de manera efectiva?**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTDiRU9Kv6p6"
      },
      "source": [
        "## üü´ 6. Reflexi√≥n Final\n",
        "‚úÖ LangChain v1.0 organiza la interacci√≥n con LLMs en tres componentes: PromptTemplate, ChatModel y Chains.\n",
        "- Usar Gemini 2.5 Pro permite aprovechar la potencia de los modelos de Google para tareas conversacionales y educativas.\n",
        "- La modularidad de LangChain facilita construir sistemas m√°s grandes (retrievers, memoria, agentes, etc.)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}