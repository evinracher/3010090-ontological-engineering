{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evinracher/3010090-ontological-engineering/blob/main/week2/2_02_ParametrosRespuesta_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a41b9897",
      "metadata": {
        "id": "a41b9897"
      },
      "source": [
        "# Temperatura, M√°x. Tokens y Streaming (Gemini)\n",
        "\n",
        "> Versi√≥n adaptada a la API de **Google Gemini** y traducida al espa√±ol.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0ff1c7",
      "metadata": {
        "id": "6c0ff1c7"
      },
      "source": [
        "¬°Bienvenido de nuevo!\n",
        "\n",
        "En secciones anteriores hablamos de los **roles** que normalmente se usan al conversar con un modelo (por ejemplo, instrucciones tipo *system* y mensajes del *usuario*), y vimos c√≥mo esas instrucciones pueden cambiar dr√°sticamente el estilo del asistente (por ejemplo, un bot sarc√°stico).\n",
        "\n",
        "En esta lecci√≥n trabajaremos tres par√°metros que afectan la respuesta del modelo:\n",
        "\n",
        "<ul>\n",
        "  <li><b>M√°ximo de tokens de salida</b> (longitud de la respuesta),</li>\n",
        "  <li><b>Temperatura</b> (aleatoriedad / creatividad), y</li>\n",
        "  <li><b>Streaming</b> (ver la respuesta ‚Äúen vivo‚Äù, por partes).</li>\n",
        "</ul>\n",
        "\n",
        "Empecemos por la longitud (tokens de salida).  \n",
        "En Gemini, el par√°metro equivalente se configura como <code>max_output_tokens</code> dentro de <code>GenerationConfig</code>.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Obtener la API key desde userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Opcional: Guardarla como variable de entorno\n",
        "os.environ['GOOGLE_API_KEY'] = api_key\n",
        "\n",
        "# Verificar que se haya cargado correctamente\n",
        "print(\"API Key cargada:\", \"S√≠\" if api_key else \"No\")\n",
        "print(\"Primeros caracteres:\", api_key[:10] if api_key else \"No encontrada\")\n"
      ],
      "metadata": {
        "id": "v8vepXj-rsQW"
      },
      "id": "v8vepXj-rsQW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dff58757",
      "metadata": {
        "id": "dff58757"
      },
      "source": [
        "## 0) Instalaci√≥n (solo la primera vez en Colab)\n",
        "Ejecuta esta celda si est√°s en Google Colab o si no tienes instalada la librer√≠a.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# üü¶ 1. Instalaci√≥n y Configuraci√≥n Inicial\n",
        "# =============================================================\n",
        "!pip install \"langchain>=1,<2\" langchain-core langchain-google-genai"
      ],
      "metadata": {
        "id": "ua4BEUO-v3-S"
      },
      "id": "ua4BEUO-v3-S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "102a97db",
      "metadata": {
        "id": "102a97db"
      },
      "source": [
        "## 3) Crear el modelo Gemini\n",
        "\n",
        "> Nota: el nombre exacto del modelo puede variar seg√∫n disponibilidad en tu cuenta. Si uno falla, prueba con `gemini-2.5-flash`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e75cd8c",
      "metadata": {
        "id": "6e75cd8c"
      },
      "outputs": [],
      "source": [
        "# Ya esta deprecated. Se usa por mostrar un ejemplo. Se actualizar√° en un futuro\n",
        "!pip -q install -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e39b0c",
      "metadata": {
        "id": "34e39b0c"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "MODEL_NAME = \"gemini-2.5-flash\"  # alternativo: \"gemini-2.5-flash-lite\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6021abd7",
      "metadata": {
        "id": "6021abd7"
      },
      "source": [
        "## 4) Ejemplo 1: M√°ximo de tokens (salida)\n",
        "\n",
        "Vamos a recrear la idea del bot ‚ÄúMarv‚Äù, con una personalidad sarc√°stica.\n",
        "\n",
        "Pregunta del usuario:\n",
        "*¬øPodr√≠as explicar brevemente qu√© es un agujero negro?*\n",
        "\n",
        "Primero, permitamos una respuesta de hasta **250 tokens** de salida.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraci√≥n del modelo\n",
        "system_instruction = (\n",
        "    \"Eres Marv, un chatbot que responde preguntas con un tono sarc√°stico (sin insultar). \"\n",
        "    \"Aun as√≠, procura ser correcto y claro.\"\n",
        ")\n",
        "\n",
        "marv = genai.GenerativeModel(\n",
        "    model_name=MODEL_NAME,   # por ejemplo: \"gemini-2.5-flash\"\n",
        "    system_instruction=system_instruction\n",
        ")\n",
        "\n",
        "prompt_usuario = \"¬øPodr√≠as explicar brevemente qu√© es un agujero negro?\"\n",
        "\n",
        "config_250 = GenerationConfig(max_output_tokens=250\n",
        "\n",
        ")\n",
        "\n",
        "# Generaci√≥n\n",
        "respuesta = marv.generate_content(\n",
        "    prompt_usuario,\n",
        "    generation_config=config_250\n",
        ")\n",
        "\n",
        "# üëâ SOLO EL TEXTO\n",
        "print(respuesta.text)"
      ],
      "metadata": {
        "id": "UXkBb6esyhVq"
      },
      "id": "UXkBb6esyhVq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "49cd5fb2",
      "metadata": {
        "id": "49cd5fb2"
      },
      "source": [
        "Como era de esperarse, nuestro bot sarc√°stico responde‚Ä¶ a su manera üôÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e83c8301",
      "metadata": {
        "id": "e83c8301"
      },
      "source": [
        "Ahora bajemos el l√≠mite a **50 tokens** y observemos el cambio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6510928e",
      "metadata": {
        "id": "6510928e"
      },
      "outputs": [],
      "source": [
        "config_50 = GenerationConfig(max_output_tokens=50)\n",
        "\n",
        "respuesta = marv.generate_content(prompt_usuario, generation_config=config_50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1a4e1d5",
      "metadata": {
        "id": "f1a4e1d5"
      },
      "outputs": [],
      "source": [
        "# üëâ SOLO EL TEXTO\n",
        "print(respuesta.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05a4bfd8",
      "metadata": {
        "id": "05a4bfd8"
      },
      "source": [
        "Esta vez la respuesta es mucho m√°s corta (y puede quedar incompleta). As√≠ que hay que tener cuidado de no limitar demasiado el modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40f8e65d",
      "metadata": {
        "id": "40f8e65d"
      },
      "source": [
        "Volvamos a **250 tokens** y pasemos a **temperatura**.\n",
        "\n",
        "La temperatura suele tomar valores entre 0 y 2 (dependiendo del proveedor/SDK). Valores mayores aumentan la aleatoriedad. Aunque muchas veces el valor por defecto funciona bien, conviene probar los extremos.\n",
        "\n",
        "Ahora quitaremos el ‚Äúsarcamo‚Äù y haremos un bot **educativo**. Ajusta la temperatura a **0** para una salida m√°s determinista.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ceb5835",
      "metadata": {
        "id": "2ceb5835"
      },
      "outputs": [],
      "source": [
        "profe = genai.GenerativeModel(\n",
        "    model_name=MODEL_NAME,\n",
        "    system_instruction=\"Eres un profesor universitario. Explicas con claridad y con ejemplos simples.\"\n",
        ")\n",
        "\n",
        "config_temp0 = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    max_output_tokens=250\n",
        ")\n",
        "\n",
        "respuesta = profe.generate_content(prompt_usuario, generation_config=config_temp0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b90ef70c",
      "metadata": {
        "id": "b90ef70c"
      },
      "outputs": [],
      "source": [
        "print(respuesta.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cef1d86",
      "metadata": {
        "id": "6cef1d86"
      },
      "source": [
        "Obtenemos una respuesta informativa.\n",
        "\n",
        "Ahora vuelve a ejecutar la celda anterior (la que define `respuesta`) y observa el resultado.  \n",
        "Con **temperatura baja**, las respuestas tienden a ser muy parecidas (aunque no id√©nticas), lo cual es √∫til en contextos educativos o cuando buscas consistencia.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "568eea34",
      "metadata": {
        "id": "568eea34"
      },
      "source": [
        "Ahora subamos la temperatura al m√°ximo (por ejemplo, **2.0**) y comparemos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a5f5d6",
      "metadata": {
        "id": "f0a5f5d6"
      },
      "outputs": [],
      "source": [
        "config_temp2 = GenerationConfig(\n",
        "    temperature=2.0,\n",
        "    max_output_tokens=250\n",
        ")\n",
        "\n",
        "respuesta = profe.generate_content(prompt_usuario, generation_config=config_temp2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e431bb",
      "metadata": {
        "id": "30e431bb"
      },
      "outputs": [],
      "source": [
        "print(respuesta.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f347ac4",
      "metadata": {
        "id": "2f347ac4"
      },
      "source": [
        "Probablemente notar√°s que la respuesta se vuelve m√°s creativa‚Ä¶ y tambi√©n m√°s propensa a desviarse o a ‚Äúrellenar‚Äù con cosas menos √∫tiles. En general, temperaturas muy altas rara vez ayudan en tareas de explicaci√≥n t√©cnica.\n",
        "\n",
        "Volvamos a temperatura 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fee782e5",
      "metadata": {
        "id": "fee782e5"
      },
      "outputs": [],
      "source": [
        "respuesta = profe.generate_content(prompt_usuario, generation_config=config_temp0)\n",
        "print(respuesta.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74804678",
      "metadata": {
        "id": "74804678"
      },
      "source": [
        "## 5) Semilla (seed) para mayor reproducibilidad\n",
        "\n",
        "Otro par√°metro que busca aumentar la consistencia es **seed** (semilla), similar a lo que hacemos en ML para reproducibilidad.\n",
        "\n",
        "En LLMs la determinaci√≥n **no est√° garantizada** al 100%, pero un `seed` fijo puede ayudar a que las salidas sean lo m√°s parecidas posible.\n",
        "\n",
        "> Ojo: el soporte de `seed` puede variar por versi√≥n del SDK / backend. Por eso lo aplicaremos con un enfoque ‚Äúseguro‚Äù.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59523ba3",
      "metadata": {
        "id": "59523ba3"
      },
      "outputs": [],
      "source": [
        "# Intentamos fijar seed (si el backend/SDK lo soporta)\n",
        "def make_config_temp0_seed(seed: int = 365):\n",
        "    try:\n",
        "        return GenerationConfig(\n",
        "            temperature=0.0,\n",
        "            max_output_tokens=250,\n",
        "            seed=seed\n",
        "        )\n",
        "    except TypeError:\n",
        "        # Si tu versi√≥n no soporta 'seed', caemos a configuraci√≥n sin seed\n",
        "        return GenerationConfig(\n",
        "            temperature=0.0,\n",
        "            max_output_tokens=250\n",
        "        )\n",
        "\n",
        "config_temp0_seed = make_config_temp0_seed(365)\n",
        "\n",
        "respuesta = profe.generate_content(prompt_usuario, generation_config=config_temp0_seed)\n",
        "print(respuesta.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dc35b7a",
      "metadata": {
        "id": "7dc35b7a"
      },
      "source": [
        "## 6) Streaming (respuesta en vivo)\n",
        "\n",
        "Una manera de hacer un chatbot m√°s ‚Äúreactivo‚Äù es imprimir la salida **mientras se genera**, en lugar de esperar a que termine.\n",
        "\n",
        "En Gemini, puedes usar `stream=True` para obtener un iterador con ‚Äútrozos‚Äù (chunks) de la respuesta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0aabc0",
      "metadata": {
        "id": "ac0aabc0"
      },
      "outputs": [],
      "source": [
        "stream = profe.generate_content(\n",
        "    prompt_usuario,\n",
        "    generation_config=config_temp0_seed,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "stream"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d795d361",
      "metadata": {
        "id": "d795d361"
      },
      "source": [
        "Lo importante es que este objeto se puede iterar con un `for`.\n",
        "Primero, veamos los objetos (chunks) que llegan:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf8719b",
      "metadata": {
        "id": "2bf8719b"
      },
      "outputs": [],
      "source": [
        "for chunk in stream:\n",
        "    print(chunk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f65e3fe7",
      "metadata": {
        "id": "f65e3fe7"
      },
      "source": [
        "Ahora hagamos *streaming* del texto: (vuelve a ejecutar la celda que define `stream` para regenerarlo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c717098",
      "metadata": {
        "id": "2c717098"
      },
      "outputs": [],
      "source": [
        "stream = profe.generate_content(\n",
        "    prompt_usuario,\n",
        "    generation_config=config_temp0_seed,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    # Cada chunk suele traer una parte del texto\n",
        "    if getattr(chunk, \"text\", None):\n",
        "        print(chunk.text, end=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b76d5b",
      "metadata": {
        "id": "02b76d5b"
      },
      "source": [
        "¬°Listo! Has hecho streaming de la respuesta en pantalla üôÇ  \n",
        "\n",
        "## Cierre\n",
        "\n",
        "Con esto termina esta introducci√≥n corta a la **API de Gemini** (en el estilo del notebook original).\n",
        "\n",
        "La gracia de trabajar con una API es que te da control fino sobre:\n",
        "- longitud de salida,\n",
        "- creatividad,\n",
        "- consistencia,\n",
        "- y experiencia de usuario (streaming).\n",
        "\n",
        "En las siguientes secciones (por ejemplo, con **LangChain** / **LangGraph**) podr√°s construir flujos m√°s complejos: herramientas, memoria,agentes, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d89e59",
      "metadata": {
        "id": "82d89e59"
      },
      "outputs": [],
      "source": [
        "# Fin del notebook ‚úÖ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}