{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evinracher/3010090-ontological-engineering/blob/main/week2/2_03_Taller_Memoria_Gemini25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c7f4d8",
      "metadata": {
        "id": "57c7f4d8"
      },
      "source": [
        "# Taller: Memoria en LangChain 1.0 usando gemini-2.5-flash-lite\n",
        "Este cuaderno explica cÃ³mo funciona la memoria en LangChain 1.0 mediante `RunnableWithMessageHistory`, `ChatMessageHistory` y Gemini."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cea25ad9",
      "metadata": {
        "id": "cea25ad9"
      },
      "source": [
        "## Configura tu clave de Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "53521754",
      "metadata": {
        "id": "53521754",
        "outputId": "18d5cdd6-fddf-4334-fb8e-53917a3411d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key cargada: SÃ­\n",
            "Primeros caracteres: AIzaSyDx4e\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Obtener la API key desde userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Opcional: Guardarla como variable de entorno\n",
        "os.environ['GOOGLE_API_KEY'] = api_key\n",
        "\n",
        "# Verificar que se haya cargado correctamente\n",
        "print(\"API Key cargada:\", \"SÃ­\" if api_key else \"No\")\n",
        "print(\"Primeros caracteres:\", api_key[:10] if api_key else \"No encontrada\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b61ec5",
      "metadata": {
        "id": "d6b61ec5"
      },
      "source": [
        "## InstalaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain langchain_community langchain_core langchain-google-genai\n",
        "\n",
        "print(\"âœ… Paquetes instalados correctamente\")"
      ],
      "metadata": {
        "id": "Lg_ESS7Q0l6P",
        "outputId": "6709ed3d-3e79-4910-98e9-522dfaefac2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Lg_ESS7Q0l6P",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/500.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m500.1/500.1 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/66.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/158.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… Paquetes instalados correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show langchain"
      ],
      "metadata": {
        "id": "k7tee2cxA4RR",
        "outputId": "aca91446-38c4-4a3b-84ad-a6f66e1b6a00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "k7tee2cxA4RR",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 1.2.10\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://docs.langchain.com/\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, langgraph, pydantic\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9c2c1b6",
      "metadata": {
        "id": "b9c2c1b6"
      },
      "source": [
        "## Cargar modelo gemini-2.5-flash-lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ac45454e",
      "metadata": {
        "id": "ac45454e",
        "outputId": "4e859627-1c76-4d46-e219-bc98be8838be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Modelo configurado: gemini-2.5-flash-lite\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "import time\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash-lite\",\n",
        "    temperature=0.3,\n",
        "    max_retries=2\n",
        ")\n",
        "\n",
        "print(\"âœ… Modelo configurado: gemini-2.5-flash-lite\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## FunciÃ³n de Manejo de Errores"
      ],
      "metadata": {
        "id": "zmWCQb4-1eEp"
      },
      "id": "zmWCQb4-1eEp"
    },
    {
      "cell_type": "code",
      "source": [
        "def invocar_con_retry(chain, input_data, config, max_intentos=3, delay=10):\n",
        "    \"\"\"Maneja automÃ¡ticamente los errores 429 con esperas progresivas.\"\"\"\n",
        "    for intento in range(max_intentos):\n",
        "        try:\n",
        "            return chain.invoke(input_data, config=config)\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            if \"429\" in error_msg or \"ResourceExhausted\" in error_msg:\n",
        "                if intento < max_intentos - 1:\n",
        "                    print(f\"âš ï¸ LÃ­mite alcanzado. Esperando {delay} segundos...\")\n",
        "                    time.sleep(delay)\n",
        "                    delay *= 2\n",
        "                else:\n",
        "                    print(\"âŒ Se agotaron los intentos. Espera unos minutos.\")\n",
        "                    raise\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "print(\"âœ… FunciÃ³n de retry configurada\")"
      ],
      "metadata": {
        "id": "UvtKg-Fg1kmB",
        "outputId": "a9659288-e8aa-4b4f-c037-93c553902d00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UvtKg-Fg1kmB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… FunciÃ³n de retry configurada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4155a36c",
      "metadata": {
        "id": "4155a36c"
      },
      "source": [
        "## Â¿QuÃ© es la memoria en LangChain 1.0?\n",
        "LangChain 1.0 *ya no usa* `ConversationBufferMemory` o `ConversationSummaryMemory`.\n",
        "\n",
        "En su lugar usa:\n",
        "\n",
        "### âœ” `RunnableWithMessageHistory`\n",
        "Un envoltorio que permite agregar historial a *cualquier* cadena, prompt o modelo.\n",
        "\n",
        "### âœ” `ChatMessageHistory`\n",
        "Objeto que guarda mensajes pasados entre usuario y asistente.\n",
        "\n",
        "### âœ” El historial es **externo**, no estÃ¡ dentro del modelo.\n",
        "Se simula un estado de conversaciÃ³n agregando mensajes previos en cada invocaciÃ³n."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurar Memoria con RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "dpy54DzA2_kV"
      },
      "id": "dpy54DzA2_kV"
    },
    {
      "cell_type": "code",
      "source": [
        "# â­ PROMPT MÃS ESTRICTO - Evita alucinaciones\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"Eres un asistente honesto y preciso.\n",
        "\n",
        "REGLAS ESTRICTAS QUE DEBES SEGUIR:\n",
        "1. SOLO puedes acceder a la informaciÃ³n que estÃ¡ en los mensajes del historial de esta conversaciÃ³n\n",
        "2. Si el historial estÃ¡ vacÃ­o o no contiene la informaciÃ³n solicitada, debes decir \"No tengo esa informaciÃ³n en el historial de esta conversaciÃ³n\"\n",
        "3. NUNCA inventes, asumas o adivines informaciÃ³n\n",
        "4. NUNCA digas que recuerdas algo si no estÃ¡ explÃ­citamente en el historial\n",
        "5. Si te preguntan sobre informaciÃ³n que no estÃ¡ en los mensajes previos, responde: \"No encuentro esa informaciÃ³n en nuestro historial de conversaciÃ³n. Â¿PodrÃ­as compartirla conmigo?\"\n",
        "\n",
        "Recuerda: Si el historial no contiene la respuesta, debes ser honesto y decir que no la tienes.\"\"\"),\n",
        "    MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "stores = {}\n",
        "\n",
        "def obtener_historial(session_id):\n",
        "    if session_id not in stores:\n",
        "        stores[session_id] = ChatMessageHistory()\n",
        "    return stores[session_id]\n",
        "\n",
        "chain_con_memoria = RunnableWithMessageHistory(\n",
        "    prompt | model,\n",
        "    obtener_historial,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"messages\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Memoria configurada con prompt ANTI-ALUCINACIÃ“N\")"
      ],
      "metadata": {
        "id": "LdOoGTpz8B5J",
        "outputId": "47ff10ca-561f-44e4-8245-6da5ac1449e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LdOoGTpz8B5J",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Memoria configurada con prompt ANTI-ALUCINACIÃ“N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROMPT DETALLADO - MÃ¡s explÃ­cito sobre recordar informaciÃ³n\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"Eres un asistente amable y servicial con excelente memoria.\n",
        "\n",
        "INSTRUCCIONES IMPORTANTES:\n",
        "- Debes recordar TODA la informaciÃ³n que el usuario te comparta\n",
        "- Cuando el usuario te pregunte sobre informaciÃ³n previa, busca en el historial de la conversaciÃ³n\n",
        "- Si el usuario pregunta \"Â¿En quÃ© trabajo?\" o similar, responde basÃ¡ndote en lo que Ã‰L te dijo anteriormente\n",
        "- NUNCA inventes informaciÃ³n que el usuario no te haya dicho\n",
        "- SIEMPRE usa la informaciÃ³n del historial de esta conversaciÃ³n especÃ­fica\n",
        "\n",
        "Recuerda: Cada conversaciÃ³n es independiente y debes recordar lo que el usuario de ESTA sesiÃ³n te ha dicho.\"\"\"),\n",
        "    MessagesPlaceholder(variable_name=\"messages\"), # â¬…ï¸ ESTO ES CLAVE\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "stores = {}\n",
        "\n",
        "def obtener_historial(session_id):\n",
        "    \"\"\"Obtiene o crea un historial para una sesiÃ³n especÃ­fica.\"\"\"\n",
        "    if session_id not in stores:\n",
        "        stores[session_id] = ChatMessageHistory()\n",
        "    return stores[session_id]\n",
        "\n",
        "# Crear cadena con memoria\n",
        "chain_con_memoria = RunnableWithMessageHistory(\n",
        "    prompt | model,\n",
        "    obtener_historial,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"messages\" # â¬…ï¸ Debe coincidir con MessagesPlaceholder\n",
        ")\n",
        "\n",
        "print(\"âœ… Memoria configurada con prompt DETALLADO y con ANTI-ALUCINACIÃ“N\")"
      ],
      "metadata": {
        "id": "MXqCK4XmlFNZ",
        "outputId": "484dc36c-2088-485e-e980-e57de69147d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MXqCK4XmlFNZ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Memoria configurada con prompt DETALLADO y con ANTI-ALUCINACIÃ“N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d96ee75",
      "metadata": {
        "id": "1d96ee75"
      },
      "source": [
        "## Ejemplo 1 - Memoria BÃ¡sica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "89bb86f1",
      "metadata": {
        "id": "89bb86f1",
        "outputId": "ea422871-97e3-4daa-c808-351f7f829dce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "EJEMPLO 1: MEMORIA BÃSICA\n",
            "==================================================\n",
            "\n",
            "ğŸ¤– Primera interacciÃ³n:\n",
            "Asistente: Â¡Hola Carlos! Es un placer conocerte. Me alegra saber que estudias Deep Learning. Es un campo fascinante.\n",
            "\n",
            "Â¿Hay algo en particular sobre Deep Learning en lo que te pueda ayudar hoy?\n",
            "\n",
            "â³ Esperando 5 segundos...\n",
            "\n",
            "ğŸ¤– Segunda interacciÃ³n (prueba de memoria):\n",
            "Asistente: Claro que sÃ­, Carlos. Me dijiste que tu nombre es Carlos y que estudias Deep Learning.\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*50)\n",
        "print(\"EJEMPLO 1: MEMORIA BÃSICA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "session_id = \"usuario1\"\n",
        "\n",
        "try:\n",
        "    print(\"\\nğŸ¤– Primera interacciÃ³n:\")\n",
        "    respuesta1 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"Hola, mi nombre es Carlos y estudio Deep Learning.\"},\n",
        "        {\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    print(\"Asistente:\", respuesta1.content)\n",
        "\n",
        "    print(\"\\nâ³ Esperando 5 segundos...\")\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(\"\\nğŸ¤– Segunda interacciÃ³n (prueba de memoria):\")\n",
        "    respuesta2 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"Â¿Recuerdas mi nombre y quÃ© estudio?\"},\n",
        "        {\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    print(\"Asistente:\", respuesta2.content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error: {e}\")\n",
        "    print(\"\\nğŸ’¡ Espera 1-2 minutos antes de volver a intentar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7f18bc0",
      "metadata": {
        "id": "b7f18bc0"
      },
      "source": [
        "## Ejemplo 2: Memoria por mÃºltiples usuarios"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EJEMPLO 2: MÃšLTIPLES USUARIOS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    # Usuario A\n",
        "    print(\"\\nğŸ‘¤ SesiÃ³n A - Ana:\")\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(\"Ana dice: Soy Ana y mi color favorito es el azul.\")\n",
        "    respA1 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"Soy Ana y mi color favorito es el azul.\"},\n",
        "        {\"configurable\": {\"session_id\": \"A\"}}\n",
        "    )\n",
        "    print(\"ğŸ¤– Asistente (A):\", respA1.content)\n",
        "\n",
        "    time.sleep(5)\n",
        "    print(\"\\nAna pregunta: Â¿CuÃ¡l es mi color favorito?\")\n",
        "    respA2 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"Â¿CuÃ¡l es mi color favorito?\"},\n",
        "        {\"configurable\": {\"session_id\": \"A\"}}\n",
        "    )\n",
        "    print(\"ğŸ¤– Asistente (A):\", respA2.content)\n",
        "\n",
        "    # Usuario B\n",
        "    print(\"\\nğŸ‘¤ SesiÃ³n B - Pedro:\")\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(\"Pedro dice: Me llamo Pedro y trabajo en Inteligencia Artificial.\")\n",
        "    respB1 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"Me llamo Pedro y trabajo en Inteligencia Artificial.\"},\n",
        "        {\"configurable\": {\"session_id\": \"B\"}}\n",
        "    )\n",
        "    print(\"ğŸ¤– Asistente (B):\", respB1.content)\n",
        "\n",
        "    time.sleep(5)\n",
        "    print(\"\\nPedro pregunta: Â¿En quÃ© trabajo yo?\")  # â¬…ï¸ MÃ¡s explÃ­cito\n",
        "    respB2 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"Â¿En quÃ© trabajo yo?\"},  # â¬…ï¸ Cambiado para ser mÃ¡s claro\n",
        "        {\"configurable\": {\"session_id\": \"B\"}}\n",
        "    )\n",
        "    print(\"ğŸ¤– Asistente (B):\", respB2.content)\n",
        "\n",
        "    # VerificaciÃ³n adicional\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"VERIFICACIÃ“N DE MEMORIA:\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    time.sleep(5)\n",
        "    print(\"\\nAna pregunta: Â¿Recuerdas mi nombre?\")\n",
        "    respA3 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"Â¿Recuerdas mi nombre?\"},\n",
        "        {\"configurable\": {\"session_id\": \"A\"}}\n",
        "    )\n",
        "    print(\"ğŸ¤– Asistente (A):\", respA3.content)\n",
        "\n",
        "    time.sleep(5)\n",
        "    print(\"\\nPedro pregunta: Â¿CuÃ¡l es mi nombre y profesiÃ³n?\")\n",
        "    respB3 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"Â¿CuÃ¡l es mi nombre y profesiÃ³n?\"},\n",
        "        {\"configurable\": {\"session_id\": \"B\"}}\n",
        "    )\n",
        "    print(\"ğŸ¤– Asistente (B):\", respB3.content)\n",
        "\n",
        "    print(\"\\nâœ… Ejemplo completado - Verifica que cada sesiÃ³n mantenga su memoria independiente\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error: {e}\")"
      ],
      "metadata": {
        "id": "wKMxOWSifjxC",
        "outputId": "aa73ccb0-7ba2-4dd9-d1dd-299d94132e26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wKMxOWSifjxC",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "EJEMPLO 2: MÃšLTIPLES USUARIOS\n",
            "==================================================\n",
            "\n",
            "ğŸ‘¤ SesiÃ³n A - Ana:\n",
            "Ana dice: Soy Ana y mi color favorito es el azul.\n",
            "ğŸ¤– Asistente (A): Â¡Hola Ana! Es un placer conocerte. RecordarÃ© que tu color favorito es el azul.\n",
            "\n",
            "Ana pregunta: Â¿CuÃ¡l es mi color favorito?\n",
            "ğŸ¤– Asistente (A): Tu color favorito es el azul.\n",
            "\n",
            "ğŸ‘¤ SesiÃ³n B - Pedro:\n",
            "Pedro dice: Me llamo Pedro y trabajo en Inteligencia Artificial.\n",
            "ğŸ¤– Asistente (B): Â¡Hola Pedro! Es un placer conocerte. Entonces, trabajas en Inteligencia Artificial.\n",
            "\n",
            "Pedro pregunta: Â¿En quÃ© trabajo yo?\n",
            "ğŸ¤– Asistente (B): Me dijiste que te llamas Pedro y que trabajas en Inteligencia Artificial.\n",
            "\n",
            "--------------------------------------------------\n",
            "VERIFICACIÃ“N DE MEMORIA:\n",
            "--------------------------------------------------\n",
            "\n",
            "Ana pregunta: Â¿Recuerdas mi nombre?\n",
            "ğŸ¤– Asistente (A): SÃ­, tu nombre es Ana.\n",
            "\n",
            "Pedro pregunta: Â¿CuÃ¡l es mi nombre y profesiÃ³n?\n",
            "ğŸ¤– Asistente (B): Tu nombre es Pedro y trabajas en Inteligencia Artificial.\n",
            "\n",
            "âœ… Ejemplo completado - Verifica que cada sesiÃ³n mantenga su memoria independiente\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}