{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evinracher/3010090-ontological-engineering/blob/main/week2/2_03_Taller_Memoria_Gemini25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c7f4d8",
      "metadata": {
        "id": "57c7f4d8"
      },
      "source": [
        "# Taller: Memoria en LangChain 1.0 usando gemini-2.5-flash-lite\n",
        "Este cuaderno explica c√≥mo funciona la memoria en LangChain 1.0 mediante `RunnableWithMessageHistory`, `ChatMessageHistory` y Gemini."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cea25ad9",
      "metadata": {
        "id": "cea25ad9"
      },
      "source": [
        "## Configura tu clave de Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53521754",
      "metadata": {
        "id": "53521754"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Obtener la API key desde userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Opcional: Guardarla como variable de entorno\n",
        "os.environ['GOOGLE_API_KEY'] = api_key\n",
        "\n",
        "# Verificar que se haya cargado correctamente\n",
        "print(\"API Key cargada:\", \"S√≠\" if api_key else \"No\")\n",
        "print(\"Primeros caracteres:\", api_key[:10] if api_key else \"No encontrada\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b61ec5",
      "metadata": {
        "id": "d6b61ec5"
      },
      "source": [
        "## Instalaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain langchain_community langchain_core langchain-google-genai\n",
        "\n",
        "print(\"‚úÖ Paquetes instalados correctamente\")"
      ],
      "metadata": {
        "id": "Lg_ESS7Q0l6P"
      },
      "id": "Lg_ESS7Q0l6P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip show langchain"
      ],
      "metadata": {
        "id": "k7tee2cxA4RR"
      },
      "id": "k7tee2cxA4RR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b9c2c1b6",
      "metadata": {
        "id": "b9c2c1b6"
      },
      "source": [
        "## Cargar modelo gemini-2.5-flash-lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac45454e",
      "metadata": {
        "id": "ac45454e"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "import time\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash-lite\",\n",
        "    temperature=0.3,\n",
        "    max_retries=2\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo configurado: gemini-2.5-flash-lite\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Funci√≥n de Manejo de Errores"
      ],
      "metadata": {
        "id": "zmWCQb4-1eEp"
      },
      "id": "zmWCQb4-1eEp"
    },
    {
      "cell_type": "code",
      "source": [
        "def invocar_con_retry(chain, input_data, config, max_intentos=3, delay=10):\n",
        "    \"\"\"Maneja autom√°ticamente los errores 429 con esperas progresivas.\"\"\"\n",
        "    for intento in range(max_intentos):\n",
        "        try:\n",
        "            return chain.invoke(input_data, config=config)\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            if \"429\" in error_msg or \"ResourceExhausted\" in error_msg:\n",
        "                if intento < max_intentos - 1:\n",
        "                    print(f\"‚ö†Ô∏è L√≠mite alcanzado. Esperando {delay} segundos...\")\n",
        "                    time.sleep(delay)\n",
        "                    delay *= 2\n",
        "                else:\n",
        "                    print(\"‚ùå Se agotaron los intentos. Espera unos minutos.\")\n",
        "                    raise\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "print(\"‚úÖ Funci√≥n de retry configurada\")"
      ],
      "metadata": {
        "id": "UvtKg-Fg1kmB"
      },
      "id": "UvtKg-Fg1kmB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4155a36c",
      "metadata": {
        "id": "4155a36c"
      },
      "source": [
        "## ¬øQu√© es la memoria en LangChain 1.0?\n",
        "LangChain 1.0 *ya no usa* `ConversationBufferMemory` o `ConversationSummaryMemory`.\n",
        "\n",
        "En su lugar usa:\n",
        "\n",
        "### ‚úî `RunnableWithMessageHistory`\n",
        "Un envoltorio que permite agregar historial a *cualquier* cadena, prompt o modelo.\n",
        "\n",
        "### ‚úî `ChatMessageHistory`\n",
        "Objeto que guarda mensajes pasados entre usuario y asistente.\n",
        "\n",
        "### ‚úî El historial es **externo**, no est√° dentro del modelo.\n",
        "Se simula un estado de conversaci√≥n agregando mensajes previos en cada invocaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurar Memoria con RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "dpy54DzA2_kV"
      },
      "id": "dpy54DzA2_kV"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚≠ê PROMPT M√ÅS ESTRICTO - Evita alucinaciones\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"Eres un asistente honesto y preciso.\n",
        "\n",
        "REGLAS ESTRICTAS QUE DEBES SEGUIR:\n",
        "1. SOLO puedes acceder a la informaci√≥n que est√° en los mensajes del historial de esta conversaci√≥n\n",
        "2. Si el historial est√° vac√≠o o no contiene la informaci√≥n solicitada, debes decir \"No tengo esa informaci√≥n en el historial de esta conversaci√≥n\"\n",
        "3. NUNCA inventes, asumas o adivines informaci√≥n\n",
        "4. NUNCA digas que recuerdas algo si no est√° expl√≠citamente en el historial\n",
        "5. Si te preguntan sobre informaci√≥n que no est√° en los mensajes previos, responde: \"No encuentro esa informaci√≥n en nuestro historial de conversaci√≥n. ¬øPodr√≠as compartirla conmigo?\"\n",
        "\n",
        "Recuerda: Si el historial no contiene la respuesta, debes ser honesto y decir que no la tienes.\"\"\"),\n",
        "    MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "stores = {}\n",
        "\n",
        "def obtener_historial(session_id):\n",
        "    if session_id not in stores:\n",
        "        stores[session_id] = ChatMessageHistory()\n",
        "    return stores[session_id]\n",
        "\n",
        "chain_con_memoria = RunnableWithMessageHistory(\n",
        "    prompt | model,\n",
        "    obtener_historial,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"messages\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Memoria configurada con prompt ANTI-ALUCINACI√ìN\")"
      ],
      "metadata": {
        "id": "LdOoGTpz8B5J"
      },
      "id": "LdOoGTpz8B5J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROMPT DETALLADO - M√°s expl√≠cito sobre recordar informaci√≥n\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"Eres un asistente amable y servicial con excelente memoria.\n",
        "\n",
        "INSTRUCCIONES IMPORTANTES:\n",
        "- Debes recordar TODA la informaci√≥n que el usuario te comparta\n",
        "- Cuando el usuario te pregunte sobre informaci√≥n previa, busca en el historial de la conversaci√≥n\n",
        "- Si el usuario pregunta \"¬øEn qu√© trabajo?\" o similar, responde bas√°ndote en lo que √âL te dijo anteriormente\n",
        "- NUNCA inventes informaci√≥n que el usuario no te haya dicho\n",
        "- SIEMPRE usa la informaci√≥n del historial de esta conversaci√≥n espec√≠fica\n",
        "\n",
        "Recuerda: Cada conversaci√≥n es independiente y debes recordar lo que el usuario de ESTA sesi√≥n te ha dicho.\"\"\"),\n",
        "    MessagesPlaceholder(variable_name=\"messages\"), # ‚¨ÖÔ∏è ESTO ES CLAVE\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "stores = {}\n",
        "\n",
        "def obtener_historial(session_id):\n",
        "    \"\"\"Obtiene o crea un historial para una sesi√≥n espec√≠fica.\"\"\"\n",
        "    if session_id not in stores:\n",
        "        stores[session_id] = ChatMessageHistory()\n",
        "    return stores[session_id]\n",
        "\n",
        "# Crear cadena con memoria\n",
        "chain_con_memoria = RunnableWithMessageHistory(\n",
        "    prompt | model,\n",
        "    obtener_historial,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"messages\" # ‚¨ÖÔ∏è Debe coincidir con MessagesPlaceholder\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Memoria configurada con prompt DETALLADO y con ANTI-ALUCINACI√ìN\")"
      ],
      "metadata": {
        "id": "MXqCK4XmlFNZ"
      },
      "id": "MXqCK4XmlFNZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1d96ee75",
      "metadata": {
        "id": "1d96ee75"
      },
      "source": [
        "## Ejemplo 1 - Memoria B√°sica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89bb86f1",
      "metadata": {
        "id": "89bb86f1"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*50)\n",
        "print(\"EJEMPLO 1: MEMORIA B√ÅSICA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "session_id = \"usuario1\"\n",
        "\n",
        "try:\n",
        "    print(\"\\nü§ñ Primera interacci√≥n:\")\n",
        "    respuesta1 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"Hola, mi nombre es Carlos y estudio Deep Learning.\"},\n",
        "        {\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    print(\"Asistente:\", respuesta1.content)\n",
        "\n",
        "    print(\"\\n‚è≥ Esperando 5 segundos...\")\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(\"\\nü§ñ Segunda interacci√≥n (prueba de memoria):\")\n",
        "    respuesta2 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"¬øRecuerdas mi nombre y qu√© estudio?\"},\n",
        "        {\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    print(\"Asistente:\", respuesta2.content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"\\nüí° Espera 1-2 minutos antes de volver a intentar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7f18bc0",
      "metadata": {
        "id": "b7f18bc0"
      },
      "source": [
        "## Ejemplo 2: Memoria por m√∫ltiples usuarios"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EJEMPLO 2: M√öLTIPLES USUARIOS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    # Usuario A\n",
        "    print(\"\\nüë§ Sesi√≥n A - Ana:\")\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(\"Ana dice: Soy Ana y mi color favorito es el azul.\")\n",
        "    respA1 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"Soy Ana y mi color favorito es el azul.\"},\n",
        "        {\"configurable\": {\"session_id\": \"A\"}}\n",
        "    )\n",
        "    print(\"ü§ñ Asistente (A):\", respA1.content)\n",
        "\n",
        "    time.sleep(5)\n",
        "    print(\"\\nAna pregunta: ¬øCu√°l es mi color favorito?\")\n",
        "    respA2 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"¬øCu√°l es mi color favorito?\"},\n",
        "        {\"configurable\": {\"session_id\": \"A\"}}\n",
        "    )\n",
        "    print(\"ü§ñ Asistente (A):\", respA2.content)\n",
        "\n",
        "    # Usuario B\n",
        "    print(\"\\nüë§ Sesi√≥n B - Pedro:\")\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(\"Pedro dice: Me llamo Pedro y trabajo en Inteligencia Artificial.\")\n",
        "    respB1 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"Me llamo Pedro y trabajo en Inteligencia Artificial.\"},\n",
        "        {\"configurable\": {\"session_id\": \"B\"}}\n",
        "    )\n",
        "    print(\"ü§ñ Asistente (B):\", respB1.content)\n",
        "\n",
        "    time.sleep(5)\n",
        "    print(\"\\nPedro pregunta: ¬øEn qu√© trabajo yo?\")  # ‚¨ÖÔ∏è M√°s expl√≠cito\n",
        "    respB2 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"¬øEn qu√© trabajo yo?\"},  # ‚¨ÖÔ∏è Cambiado para ser m√°s claro\n",
        "        {\"configurable\": {\"session_id\": \"B\"}}\n",
        "    )\n",
        "    print(\"ü§ñ Asistente (B):\", respB2.content)\n",
        "\n",
        "    # Verificaci√≥n adicional\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"VERIFICACI√ìN DE MEMORIA:\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    time.sleep(5)\n",
        "    print(\"\\nAna pregunta: ¬øRecuerdas mi nombre?\")\n",
        "    respA3 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"¬øRecuerdas mi nombre?\"},\n",
        "        {\"configurable\": {\"session_id\": \"A\"}}\n",
        "    )\n",
        "    print(\"ü§ñ Asistente (A):\", respA3.content)\n",
        "\n",
        "    time.sleep(5)\n",
        "    print(\"\\nPedro pregunta: ¬øCu√°l es mi nombre y profesi√≥n?\")\n",
        "    respB3 = invocar_con_retry(\n",
        "        chain_con_memoria,\n",
        "        {\"input\": \"¬øCu√°l es mi nombre y profesi√≥n?\"},\n",
        "        {\"configurable\": {\"session_id\": \"B\"}}\n",
        "    )\n",
        "    print(\"ü§ñ Asistente (B):\", respB3.content)\n",
        "\n",
        "    print(\"\\n‚úÖ Ejemplo completado - Verifica que cada sesi√≥n mantenga su memoria independiente\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ],
      "metadata": {
        "id": "wKMxOWSifjxC"
      },
      "id": "wKMxOWSifjxC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsGx9Qv8zT8B"
      },
      "id": "lsGx9Qv8zT8B",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}