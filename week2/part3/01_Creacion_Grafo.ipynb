{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evinracher/3010090-ontological-engineering/blob/main/week2/part3/01_Creacion_Grafo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lMehvFXQIF4"
      },
      "source": [
        "# üåê Taller Introductorio de LangGraph (Google Colab)\n",
        "### Usando LangChain 1.1 + LangGraph 1.0 + Gemini\n",
        "Este cuaderno introduce los conceptos b√°sicos de LangGraph: estado, nodos, definici√≥n del grafo, ejecuci√≥n, visualizaci√≥n y uso.\n",
        "\n",
        "El dominio de aplicaci√≥n es el contar un chiste."
      ],
      "id": "3lMehvFXQIF4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalaci√≥n de librer√≠as"
      ],
      "metadata": {
        "id": "1egS53SrY47t"
      },
      "id": "1egS53SrY47t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6kC5_-_QIF6"
      },
      "source": [
        "# Instalaci√≥n de librer√≠as\n",
        "%pip install -U langgraph langchain_google_genai"
      ],
      "id": "C6kC5_-_QIF6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn9SVZW1QIF7"
      },
      "source": [
        "## üîë Configurar API Key de Gemini"
      ],
      "id": "Cn9SVZW1QIF7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvA8JPijQIF8"
      },
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = api_key\n",
        "print('API Key cargada:', 'S√≠' if api_key else 'No')"
      ],
      "id": "TvA8JPijQIF8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZI2WHYTQIF9"
      },
      "source": [
        "## üìå Importar clases necesarias"
      ],
      "id": "0ZI2WHYTQIF9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SjzBfTaQIF9"
      },
      "source": [
        "from typing import Sequence\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END"
      ],
      "id": "-SjzBfTaQIF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sAO0iefQIGA"
      },
      "source": [
        "## ü§ñ Crear el modelo Gemini para el grafo"
      ],
      "id": "3sAO0iefQIGA"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "MODEL_ID = os.getenv(\"GEMINI_MODEL\", \"models/gemini-2.5-flash-lite\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=MODEL_ID, temperature=0.2)\n",
        "print(\"‚úÖ LLM listo:\", MODEL_ID)\n"
      ],
      "metadata": {
        "id": "Qp0wWLu-Gt39"
      },
      "id": "Qp0wWLu-Gt39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgaOC6HnQIF-"
      },
      "source": [
        "## üß© Definir el Estado\n",
        "El estado en LangGraph es un diccionario tipado (TypedDict) que contiene los datos que fluyen entre nodos."
      ],
      "id": "rgaOC6HnQIF-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vl5tWCaQIF_"
      },
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from IPython.display import Image, display\n"
      ],
      "id": "6Vl5tWCaQIF_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estado del grafo - Define la estructura de datos que se pasa entre nodos\n",
        "class Estado(TypedDict):\n",
        "    tema: str              # Tema sobre el que se crear√° el chiste\n",
        "    chiste: str            # Chiste inicial generado\n",
        "    chiste_mejorado: str   # Chiste despu√©s de la primera mejora\n",
        "    chiste_final: str      # Chiste pulido final"
      ],
      "metadata": {
        "id": "r_7b-Rb2X5J5"
      },
      "id": "r_7b-Rb2X5J5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6KPVxFVQIGB"
      },
      "source": [
        "## üîß Definir un nodo\n",
        "Cada nodo es una funci√≥n que recibe el estado y devuelve un nuevo estado.\n",
        "\n",
        "En este caso hay 4 funciones, donde cada funci√≥n representa un paso en el proceso (Nodo)."
      ],
      "id": "E6KPVxFVQIGB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRfAIhVJQIGB"
      },
      "source": [
        "def generar_chiste(estado: Estado):\n",
        "    \"\"\"Primera llamada al LLM para generar el chiste inicial\"\"\"\n",
        "    resp = llm.invoke(f\"Escribe un chiste corto sobre {estado['tema']}\")\n",
        "    return {\"chiste\": resp.content}\n",
        "\n",
        "def verificar_remate(estado: Estado):\n",
        "    \"\"\"Funci√≥n de control para verificar si el chiste tiene un buen remate\"\"\"\n",
        "    # Check if the joke ends with a common punchline punctuation\n",
        "    if estado[\"chiste\"].strip().endswith((\".\", \"!\", \"?\")):\n",
        "        return \"Aprobado\"\n",
        "    return \"Rechazado\"\n",
        "\n",
        "def mejorar_chiste(estado: Estado):\n",
        "    \"\"\"Segunda llamada al LLM para mejorar el chiste\"\"\"\n",
        "    resp = llm.invoke(\n",
        "        f\"Haz este chiste m√°s gracioso a√±adiendo juegos de palabras: {estado['chiste']}\"\n",
        "    )\n",
        "    return {\"chiste_mejorado\": resp.content}\n",
        "\n",
        "def pulir_chiste(estado: Estado):\n",
        "    \"\"\"Tercera llamada al LLM para el pulido final\"\"\"\n",
        "    resp = llm.invoke(\n",
        "        f\"A√±ade un giro sorprendente a este chiste: {estado['chiste_mejorado']}\"\n",
        "    )\n",
        "    return {\"chiste_final\": resp.content}"
      ],
      "id": "SRfAIhVJQIGB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHd9-w9iQIGB"
      },
      "source": [
        "## üèóÔ∏è Construcci√≥n del grafo (flujo de trabajo)"
      ],
      "id": "fHd9-w9iQIGB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu0WaS7nQIGB"
      },
      "source": [
        "# Construir el flujo de trabajo\n",
        "flujo_trabajo = StateGraph(Estado)\n",
        "\n",
        "# Agregar nodos al grafo\n",
        "flujo_trabajo.add_node(\"generar_chiste\", generar_chiste)\n",
        "flujo_trabajo.add_node(\"mejorar_chiste\", mejorar_chiste)\n",
        "flujo_trabajo.add_node(\"pulir_chiste\", pulir_chiste)\n",
        "\n",
        "# Agregar conexiones entre nodos\n",
        "# Conexi√≥n desde el inicio al primer nodo\n",
        "flujo_trabajo.add_edge(START, \"generar_chiste\")\n",
        "\n",
        "# Conexi√≥n condicional - decide el camino seg√∫n la calidad del chiste\n",
        "flujo_trabajo.add_conditional_edges(\n",
        "    \"generar_chiste\",           # Nodo de origen\n",
        "    verificar_remate,           # Funci√≥n que eval√∫a la condici√≥n\n",
        "    {\n",
        "        \"Rechazado\": \"mejorar_chiste\",  # Si falla, mejora el chiste\n",
        "        \"Aprobado\": END                  # Si pasa, termina aqu√≠\n",
        "    }\n",
        ")\n",
        "\n",
        "# Conexiones secuenciales para el flujo de mejora\n",
        "flujo_trabajo.add_edge(\"mejorar_chiste\", \"pulir_chiste\")\n",
        "flujo_trabajo.add_edge(\"pulir_chiste\", END)\n",
        "\n",
        "# Compilar el grafo en una cadena ejecutable\n",
        "cadena = flujo_trabajo.compile()"
      ],
      "id": "iu0WaS7nQIGB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyYTgJxdQIGC"
      },
      "source": [
        "## üëÅÔ∏è Visualizar el grafo"
      ],
      "id": "VyYTgJxdQIGC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar el flujo de trabajo\n",
        "display(Image(cadena.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "hd4eXLvwcbiL"
      },
      "id": "hd4eXLvwcbiL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWxlRfcHQIGC"
      },
      "source": [
        "## ‚ñ∂Ô∏è Probar el grafo"
      ],
      "id": "gWxlRfcHQIGC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvo5Az6RQIGC"
      },
      "source": [
        "# Ejecutar la cadena\n",
        "estado = cadena.invoke({\"tema\": \"gatos\"})\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"Chiste inicial:\")\n",
        "print(estado[\"chiste\"])\n",
        "print(\"\\n--- --- ---\\n\")\n",
        "\n",
        "if \"chiste_mejorado\" in estado:\n",
        "    print(\"Chiste mejorado:\")\n",
        "    print(estado[\"chiste_mejorado\"])\n",
        "    print(\"\\n--- --- ---\\n\")\n",
        "    print(\"Chiste final:\")\n",
        "    print(estado[\"chiste_final\"])\n",
        "else:\n",
        "    print(\"El chiste pas√≥ el control de calidad - ¬°se detect√≥ remate!\")"
      ],
      "id": "cvo5Az6RQIGC",
      "execution_count": null,
      "outputs": []
    }
  ]
}